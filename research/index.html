<!DOCTYPE html>
<html lang='en'>

<head>
  <meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='description' content='Recent research publications'>
<meta name='theme-color' content='#ffcd00'>

<meta property='og:title' content='Research • Fahim Dalvi'>
<meta property='og:description' content='Recent research publications'>
<meta property='og:url' content='https://fdalvi.github.io/research/'>
<meta property='og:site_name' content='Fahim Dalvi'>
<meta property='og:type' content='article'><meta property='article:section' content='Page'><meta name='twitter:card' content='summary'>

<meta name="generator" content="Hugo 0.37" />

  <title>Research • Fahim Dalvi</title>
  <link rel='canonical' href='https://fdalvi.github.io/research/'>
  
  
  
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png?v=almNBqOxQo">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png?v=almNBqOxQo">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png?v=almNBqOxQo">
<link rel="manifest" href="/site.webmanifest?v=almNBqOxQo">
<link rel="mask-icon" href="/safari-pinned-tab.svg?v=almNBqOxQo" color="#444444">
<link rel="shortcut icon" href="/favicon.ico?v=almNBqOxQo">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

<link rel='stylesheet' href='/assets/css/main.b8f9eb89.css'><link rel='stylesheet' href='/css/custom.css'><style>
:root{--color-accent:#ffcd00;}
</style>

<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-19417017-2', 'auto');
ga('send', 'pageview');
</script>
<script async src='//www.google-analytics.com/analytics.js'></script>

</head>


<body class='page type-page'>

  <div class='site'>

    <a class='screen-reader-text' href='#content'>Skip to Content</a>

    <div class='main'>

      <nav id='main-menu' class='main-menu' aria-label='Main Menu'>
  <div class='container'>
    
    <ul><li class='item'>
        <a href='/'>Home</a>
      </li><li class='item'>
        <a href='/projects/'>Projects</a>
      </li><li class='item current'>
        <a aria-current='page' href='/research/'>Research</a>
      </li><li class='item'>
        <a href='/teaching/'>Teaching</a>
      </li><li class='item'>
        <a href='/blog/'>Blog</a>
      </li><li class='item'>
        <a href='/fdalvi-resume.pdf'>Resume</a>
      </li></ul>
  </div>
</nav>


      <header id='header' class='header site-header'>
        <div class='container sep-after'>
          <div class='header-info'><p class='site-title title'>Fahim Dalvi</p><p class='desc site-desc'>Software Engineer | Deep Learning Researcher | Mentor</p>
          </div>
        </div>
      </header>

      <main id='content'>


<article lang='en' class='entry'>
  <header class='header entry-header'>
  <div class='container sep-after'>
    <div class='header-info'>
      <h1 class='title'>Research</h1>
      
<p class='desc'>Recent research publications</p>


    </div>
    

  </div>
</header>

  
  

  <div class='container entry-content'>
  

<h3 id="2018">2018</h3>


	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2018-06-NAACL-stream-decoding/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Incremental Decoding and Training Methods for Simultaneous Translation in Neural Machine Translation</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi*</span>, Nadir Durrani*, Hassan Sajjad, Stephan Vogel </i> </div>
		 <div class="research-author-note"> * These authors contributed equally to this work </div> 
		<div class="research-venue"><a href='http://naacl2018.org'> The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics </a></div>
		<div class="research-abstract">	
			We address the problem of simultaneous translation by modifying the Neural MT decoder to operate with dynamically built encoder and attention. We propose a tunable agent which decides the best segmentation strategy for a userdefined BLEU loss and Average Proportion (AP) constraint. Our agent outperforms previously proposed Wait-if-diff and Wait-if-worse agents (Cho and Esipova, 2016) on BLEU with a lower latency. Secondly we proposed datadriven changes to Neural MT training to better match the incremental decoding framework.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2018-06-NAACL-stream-decoding/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			<a class="research-action" href="https://github.com/fdalvi/seq2seq-attn-stream">Code</a> 
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-tex" data-lang="tex">@InProceedings{dalvi-EtAl:2018,
  author =  &#34;Dalvi, Fahim
    and Durrani, Nadir
    and Sajjad, Hassan
    and Vogel, Stephan&#34;,
    title     = {Incremental Decoding and Training Methods for Simultaneous Translation in Neural Machine Translation},
    booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    month     = {June},
    year      = {2017},
    address   = {New Orleans, Louisiana},
    publisher = {Association for Computational Linguistics},
    pages     = {XX--XX},
    url       = {http://www.aclweb.org/anthology/XX-XXXX}
}
</code></pre></div>
			</div>
		
	</div>
</div>



<h3 id="2017">2017</h3>

<p>
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2017-12-IWSLT-multidomain-nmt/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Neural Machine Translation Training in a Multi-Domain Scenario</b></div>
		<div class="research-authors"> <i> Hassan Sajjad, Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span>, Yonatan Belinkov, Stephan Vogel </i> </div>
		
		<div class="research-venue"><a href='http://workshop2017.iwslt.org'>International Workshop on Spoken Language Translation 2017</a></div>
		<div class="research-abstract">	
			In this paper, we explore alternative ways to train a neural machine translation system in a multi-domain scenario. We investigate data concatenation (with fine tuning), model stacking (multi-level fine tuning), data selection and weighted ensemble. Our findings show that the best translation quality can be achieved by building an initial system on a concatenation of available out-of-domain data and then fine-tuning it on in-domain data. Model stacking works best when training begins with the furthest out-of-domain data and the model is incrementally fine-tuned with the next furthest domain and so on. Data selection did not give the best results, but can be considered as a decent compromise between training time and translation quality. A weighted ensemble of different individual models performed better than data selection. It is beneficial in a scenario when there is no time for fine-tuning.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2017-12-IWSLT-multidomain-nmt/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2017-12-IWSLT-multidomain-nmt/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-tex" data-lang="tex">@inproceedings{sajjad2017iwslt,
  title={Neural Machine Translation Training in a Multi-Domain Scenario},
  author={Sajjad, Hassan and Durrani, Nadir and Dalvi, Fahim and Belinkov, Yonatan and Vogel, Stephan},
  booktitle={International Workshop on Spoken Language Translation},
  year={2017}
}
</code></pre></div>
			</div>
		
	</div>
</div>


	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2017-12-IWSLT-reordering-models/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Continuous Space Reordering Models for Phrase-based MT</b></div>
		<div class="research-authors"> <i> Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span> </i> </div>
		
		<div class="research-venue"><a href='http://workshop2017.iwslt.org'>International Workshop on Spoken Language Translation 2017</a></div>
		<div class="research-abstract">	
			Bilingual sequence models improve phrase-based translation and reordering by overcoming phrasal independence assumption and handling long range reordering. However, due to data sparsity, these models often fall back to very small context sizes. This problem has been previously addressed by learning sequences over generalized representations such as POS tags or word clusters. In this paper, we explore an alternative based on neural network models. More concretely we train neuralized versions of lexicalized reordering and the operation sequence models using feed-forward neural network. Our results show improvements of up to 0.6 and 0.5 BLEU points on top of the baseline German→English and English→German systems. We also observed improvements compared to the systems that used POS tags and word clusters to train these models. Because we modify the bilingual corpus to integrate reordering operations, this allows us to also train a sequence-to-sequence neural MT model having explicit reordering triggers. Our motivation was to directly enable reordering information in the encoder-decoder framework, which otherwise relies solely on the attention model to handle long range reordering. We tried both coarser and fine-grained reordering operations. However, these experiments did not yield any improvements over the baseline Neural MT systems.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2017-12-IWSLT-reordering-models/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-tex" data-lang="tex">@inproceedings{durrani2017iwslt,
  title={Continuous Space Reordering Models for Phrase-based MT},
  author={Durrani, Nadir and Dalvi, Fahim},
  booktitle={International Workshop on Spoken Language Translation},
  year={2017}
}
</code></pre></div>
			</div>
		
	</div>
</div>


	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2017-11-IJCNLP-improving-nmt/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Understanding and Improving Morphological Learning in the Neural Machine Translation Decoder</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, Stephan Vogel </i> </div>
		
		<div class="research-venue"><a href='http://ijcnlp2017.org'> 8th International Joint Conference on Natural Language Processing </a></div>
		<div class="research-abstract">	
			End-to-end training makes the neural machine translation (NMT) architecture simpler, yet elegant compared to traditional statistical machine translation (SMT). However, little is known about linguistic patterns of morphology, syntax and semantics learned during the training of NMT systems, and more importantly, which parts of the architecture are responsible for learning each of these phenomena. In this paper we i) analyze how much morphology an NMT decoder learns, and ii) investigate whether injecting target morphology into the decoder helps it produce better translations. To this end we present three methods: i) joint generation, ii) joint-data learning, and iii) multi-task learning. Our results show that explicit morphological information helps the decoder learn target language morphology and improves the translation quality by 0.2–0.6 BLEU points.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2017-11-IJCNLP-improving-nmt/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			<a class="research-action" href="/research/2017-11-IJCNLP-improving-nmt/slides.pdf">Slides</a> 
			<a class="research-action" href="https://github.com/fdalvi/seq2seq-attn-multitask">Code</a> 
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-tex" data-lang="tex">@InProceedings{I17-1015,
  author =  &#34;Dalvi, Fahim
    and Durrani, Nadir
    and Sajjad, Hassan
    and Belinkov, Yonatan
    and Vogel, Stephan&#34;,
  title =   &#34;Understanding and Improving Morphological Learning in the Neural Machine Translation Decoder&#34;,
  booktitle =   &#34;Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)&#34;,
  year =  &#34;2017&#34;,
  publisher =   &#34;Asian Federation of Natural Language Processing&#34;,
  pages =   &#34;142--151&#34;,
  location =  &#34;Taipei, Taiwan&#34;,
  url =   &#34;http://aclweb.org/anthology/I17-1015&#34;
}
</code></pre></div>
			</div>
		
	</div>
</div>


	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2017-11-IJCNLP-nmt-layers/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks</b></div>
		<div class="research-authors"> <i> Yonatan Belinkov, Lluís Màrquez, Hassan Sajjad, Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span>, James Glass </i> </div>
		
		<div class="research-venue"><a href='http://ijcnlp2017.org'> 8th International Joint Conference on Natural Language Processing </a></div>
		<div class="research-abstract">	
			While neural machine translation (NMT) models provide improved translation quality in an elegant framework, it is less clear what they learn about language. Recent work has started evaluating the quality of vector representations learned by NMT models on morphological and syntactic tasks. In this paper, we investigate the representations learned at different layers of NMT encoders. We train NMT systems on parallel data and use the models to extract features for training a classifier on two tasks: part-of-speech and semantic tagging. We then measure the performance of the classifier as a proxy to the quality of the original NMT model for the given task. Our quantitative analysis yields interesting insights regarding representation learning in NMT models. For instance, we find that higher layers are better at learning semantics while lower layers tend to be better for part-of-speech tagging. We also observe little effect of the target language on source-side representations, especially in higher quality models.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2017-11-IJCNLP-nmt-layers/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-tex" data-lang="tex">@InProceedings{I17-1001,
  author =  &#34;Belinkov, Yonatan
    and M{<span style="color:#66d9ef">\`</span>a}rquez, Llu{<span style="color:#66d9ef">\&#39;</span>i}s
    and Sajjad, Hassan
    and Durrani, Nadir
    and Dalvi, Fahim
    and Glass, James&#34;,
  title =   &#34;Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks&#34;,
  booktitle =   &#34;Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)&#34;,
  year =  &#34;2017&#34;,
  publisher =   &#34;Asian Federation of Natural Language Processing&#34;,
  pages =   &#34;1--10&#34;,
  location =  &#34;Taipei, Taiwan&#34;,
  url =   &#34;http://aclweb.org/anthology/I17-1001&#34;
}
</code></pre></div>
			</div>
		
	</div>
</div>


	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2017-07-ACL-challenging-morphology/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging</b></div>
		<div class="research-authors"> <i> Hassan Sajjad, <span class='highlight-author'>Fahim Dalvi</span>, Nadir Durrani, Ahmed Abdelali, Yonatan Belinkov, Stephan Vogel  </i> </div>
		
		<div class="research-venue"><a href='http://acl2017.org'>55th Annual Meeting of the Association for Computational Linguistics</a></div>
		<div class="research-abstract">	
			Word segmentation plays a pivotal role in improving any Arabic NLP application. Therefore, a lot of research has been spent in improving its accuracy. Off-the-shelf tools, however, are: i) complicated to use and ii) domain/dialect dependent. We explore three language-independent alternatives to morphological segmentation using: i) data-driven sub-word units, ii) characters as a unit of learning, and iii) word embeddings learned using a character CNN (Convolution Neural Network). On the tasks of Machine Translation and POS tagging, we found these methods to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2017-07-ACL-challenging-morphology/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2017-07-ACL-challenging-morphology/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-tex" data-lang="tex">@InProceedings{P17-2095,
  author =  &#34;Sajjad, Hassan
    and Dalvi, Fahim
    and Durrani, Nadir
    and Abdelali, Ahmed
    and Belinkov, Yonatan
    and Vogel, Stephan&#34;,
  title =   &#34;Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging&#34;,
  booktitle =   &#34;Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)&#34;,
  year =  &#34;2017&#34;,
  publisher =   &#34;Association for Computational Linguistics&#34;,
  pages =   &#34;601--607&#34;,
  location =  &#34;Vancouver, Canada&#34;,
  doi =   &#34;10.18653/v1/P17-2095&#34;,
  url =   &#34;http://www.aclweb.org/anthology/P17-2095&#34;
}</code></pre></div>
			</div>
		
	</div>
</div>


	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2017-07-ACL-nmt-morphology/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>What do Neural Machine Translation Models Learn about Morphology?</b></div>
		<div class="research-authors"> <i> Yonatan Belinkov, Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span>, Hassan Sajjad, James Glass   </i> </div>
		
		<div class="research-venue"><a href='http://acl2017.org'>55th Annual Meeting of the Association for Computational Linguistics</a></div>
		<div class="research-abstract">	
			Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2017-07-ACL-nmt-morphology/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2017-07-ACL-nmt-morphology/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-tex" data-lang="tex">@InProceedings{P17-1080,
  author =  &#34;Belinkov, Yonatan
    and Durrani, Nadir
    and Dalvi, Fahim
    and Sajjad, Hassan
    and Glass, James&#34;,
  title =   &#34;What do Neural Machine Translation Models Learn about Morphology?&#34;,
  booktitle =   &#34;Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&#34;,
  year =  &#34;2017&#34;,
  publisher =   &#34;Association for Computational Linguistics&#34;,
  pages =   &#34;861--872&#34;,
  location =  &#34;Vancouver, Canada&#34;,
  doi =   &#34;10.18653/v1/P17-1080&#34;,
  url =   &#34;http://www.aclweb.org/anthology/P17-1080&#34;
}
</code></pre></div>
			</div>
		
	</div>
</div>


	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2017-04-EACL-Demo/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>QCRI&#39;s Live Speech Translation System</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Yifan Zhang, Sameer Khurana, Nadir Durrani, Hassan Sajjad Ahmed Abdelali, Hamdy Mubarak, Ahmed Ali, Stephan Vogel </i> </div>
		
		<div class="research-venue"><a href='http://eacl2017.org'>15th Conference of the European Chapter of the Association for Computational Linguistics</a></div>
		<div class="research-abstract">	
			We present QCRI’s Arabic-to-English speech translation system. It features modern web technologies to capture live audio, and broadcasts Arabic transcriptions and English translations simultaneously. Our Kaldi-based ASR system uses the Time Delay Neural Network architecture, while our Machine Translation (MT) system uses both phrase-based and neural frameworks. Although our neural MT system is slower than the phrase-based system, it produces significantly better translations and is memory efficient.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2017-04-EACL-Demo/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2017-04-EACL-Demo/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-tex" data-lang="tex">@InProceedings{E17-3016,
  author =  &#34;Dalvi, Fahim
    and Zhang, Yifan
    and Khurana, Sameer
    and Durrani, Nadir
    and Sajjad, Hassan
    and Abdelali, Ahmed
    and Mubarak, Hamdy
    and Ali, Ahmed
    and Vogel, Stephan&#34;,
  title =   &#34;QCRI Live Speech Translation System&#34;,
  booktitle =   &#34;Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics&#34;,
  year =  &#34;2017&#34;,
  publisher =   &#34;Association for Computational Linguistics&#34;,
  pages =   &#34;61--64&#34;,
  location =  &#34;Valencia, Spain&#34;,
  url =   &#34;http://aclweb.org/anthology/E17-3016&#34;
}
</code></pre></div>
			</div>
		
	</div>
</div>

</p>

<h3 id="2016">2016</h3>

<p>
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2016-12-DSL-Dialect-ID/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>QCRI @ DSL 2016: Spoken Arabic Dialect Identification Using Textual Features</b></div>
		<div class="research-authors"> <i> Mohamed Eldesouki, <span class='highlight-author'>Fahim Dalvi</span>, Hassan Sajjad, and Kareem Darwish </i> </div>
		
		<div class="research-venue"><a href='http://ttg.uni-saarland.de/vardial2016/'>The Third Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial3)</a></div>
		<div class="research-abstract">	
			The paper describes the QCRI submissions to the shared task of automatic Arabic dialect classification into 5 Arabic variants, namely Egyptian, Gulf, Levantine, North-African (Maghrebi), and Modern Standard Arabic (MSA). The relatively small training set is automatically generated from an ASR system. To avoid over-fitting on such small data, we selected and designed features that capture the morphological essence of the different dialects. We submitted four runs to the Arabic sub-task. For all runs, we used a combined feature vector of character bigrams, trigrams, 4-grams, and 5-grams. We tried several machine-learning algorithms, namely Logistic Regres- sion, Naive Bayes, Neural Networks, and Support Vector Machines (SVM) with linear and string kernels. Our submitted runs used SVM with a linear kernel. In the closed submission, we got the best accuracy of 0.5136 and the third best weighted F1 score, with a difference of less than 0.002 from the best system.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2016-12-DSL-Dialect-ID/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-tex" data-lang="tex">@InProceedings{W16-4828,
  author =  &#34;Eldesouki, Mohamed
    and Dalvi, Fahim
    and Sajjad, Hassan
    and Darwish, Kareem&#34;,
  title =   &#34;QCRI {<span style="color:#66d9ef">\$</span>}@{<span style="color:#66d9ef">\$</span>} DSL 2016: Spoken Arabic Dialect Identification Using Textual Features&#34;,
  booktitle =   &#34;Proceedings of the Third Workshop on NLP for Similar Languages, Varieties      and Dialects (VarDial3)    &#34;,
  year =  &#34;2016&#34;,
  publisher =   &#34;The COLING 2016 Organizing Committee&#34;,
  pages =   &#34;221--226&#34;,
  location =  &#34;Osaka, Japan&#34;,
  url =   &#34;http://www.aclweb.org/anthology/W16-4828&#34;
}
</code></pre></div>
			</div>
		
	</div>
</div>


	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2016-12-IWSLT/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>QCRI Machine Translation Systems for IWSLT 16</b></div>
		<div class="research-authors"> <i> Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span>, Hassan Sajjad, Stephan Vogel </i> </div>
		
		<div class="research-venue"><a href='http://workshop2016.iwslt.org'>International Workshop on Spoken Language Translation 2016</a></div>
		<div class="research-abstract">	
			This paper describes QCRI’s machine translation systems for the IWSLT 2016 evaluation campaign. We participated in the Arabic→English and English→Arabic tracks. We built both Phrase-based and Neural machine translation models, in an effort to probe whether the newly emerged NMT framework surpasses the traditional phrase-based systems in Arabic-English language pairs. We trained a very strong phrase-based system including, a big language model, the Operation Sequence Model, Neural Network Joint Model and Class-based models along with different domain adaptation techniques such as MML filtering, mixture modeling and using fine tuning over NNJM model. However, a Neural MT system, trained by stacking data from different genres through fine-tuning, and applying ensemble over 8 models, beat our very strong phrase-based system by a significant 2 BLEU points margin in Arabic→English direction. We did not obtain similar gains in the other direction but were still able to outperform the phrase-based system. We also applied system combination on phrase-based and NMT outputs.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2016-12-IWSLT/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2016-12-IWSLT/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			<a class="research-action" href="/research/2016-12-IWSLT/slides.pdf">Slides</a> 
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-tex" data-lang="tex">@inproceedings{durrani2016iwslt,
  title={QCRI Machine Translation Systems for IWSLT 16},
  author={Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Vogel, Stephan},
  booktitle={International Workshop on Spoken Language Translation},
  year={2016}
}
</code></pre></div>
			</div>
		
	</div>
</div>

</p>

<h3 id="unpublished-works">Unpublished works</h3>

<p><em>A list of unpublished work that resulted from student research or class projects</em></p>

<p>
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2016-stanford-ee267/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>VirtualWars: Towards a More Immersive VR Experience</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Tariq Patanam </i> </div>
		
		<div class="research-venue"><a href='https://stanford.edu/class/ee267/'>EE267: Virtual Reality, Stanford, 2016</a></div>
		<div class="research-abstract">	
			Ensuring that virtual reality experiences are immersive is key to ensuring the success of VR and even VR. However, despite impressive commercial advancements from the Oculus Rift to the HTC Vive, a number of inherent limitations remain when comparing virtual experiences to real experiences: field of view, limb (mainly hand) tracking, position tracking in the world, haptic feedback, and more. In this study we seek to test a number of creative workarounds to create a fully immersive experience with current technological limitations. We found that overall, immersive experiences could be created, but because of the limitations of the technology, limitations had to be imposed on the virtual world such as how the content had to be presented (interactively and not passively), how objects were destroyed, and more.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/unpublished-2016-stanford-ee267/report.pdf">PDF</a> 
			<a class="research-action" href="/research/unpublished-2016-stanford-ee267/poster.pdf">Poster</a> 
			
			
			
		</div>
		
	</div>
</div>


	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2016-stanford-231n/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>DeepFace: Face Generation using Deep Learning</b></div>
		<div class="research-authors"> <i> Hardie Cate, <span class='highlight-author'>Fahim Dalvi</span>, Zeshan Hussain </i> </div>
		
		<div class="research-venue"><a href='http://cs231n.stanford.edu'>CS231n: Convolutional Neural Networks for Visual Recognition, Stanford, 2016</a></div>
		<div class="research-abstract">	
			Convolutional neural networks (CNNs) are powerful tools for image classification and object detection, but they can also be used to generate images. For our project, we use CNNs to create a face generation system. Given a set of desired facial characteristics, we produce a well-formed face that matches these attributes. Potential facial char- acteristics fall within the general categories of raw at- tributes (e.g., big nose, brown hair, etc.), ethnicity (e.g., white, black, Indian), and accessories (e.g. sunglasses, hat, etc.). In our face generation system, we fine-tune a convolutional network pre-trained on faces to create a binary classification system for the potential facial charac- teristics. We then employ a novel technique that models feature activations as a custom Gaussian Mixture Model in order to identify relevant features for feature inversion. Our face generation system has many potential uses, in- cluding identifying suspects in law enforcement settings.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/unpublished-2016-stanford-231n/report.pdf">PDF</a> 
			<a class="research-action" href="/research/unpublished-2016-stanford-231n/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-tex" data-lang="tex">@article{DBLP:journals/corr/CateDH17a,
  author    = {Hardie Cate and
               Fahim Dalvi and
               Zeshan Hussain},
  title     = {DeepFace: Face Generation using Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1701.01876},
  year      = {2017},
  url       = {http://arxiv.org/abs/1701.01876},
  archivePrefix = {arXiv},
  eprint    = {1701.01876},
  timestamp = {Wed, 07 Jun 2017 14:40:49 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/CateDH17a},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}</code></pre></div>
			</div>
		
	</div>
</div>


	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2015-stanford-229/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Sign Language Recognition using Temporal Classification</b></div>
		<div class="research-authors"> <i> Hardie Cate, <span class='highlight-author'>Fahim Dalvi</span>, Zeshan Hussain </i> </div>
		
		<div class="research-venue"><a href='http://cs229.stanford.edu'>CS229: Machine Learning, Stanford, 2015</a></div>
		<div class="research-abstract">	
			In the US alone, there are approximately 900,000 hearing impaired people whose primary mode of conversation is sign language. For these people, communication with non-signers is a daily struggle, and they are often disadvantaged when it comes to finding a job, accessing health care, etc. There are a few emerging technologies aimed at overcoming these communication barriers, but most existing solutions rely on cameras to translate sign language into vocal language. While these solutions are promising, they require the hearing impaired person to carry the technology with him/her or for a proper environment to be set up for translation. One alternative is to move the technology onto the person’s body. Devices like the Myo armband available in the market today enable us to collect data about the position of the user’s hands and fingers over time. Since each sign is roughly a combination of gestures across time, we can use these technologies for sign language translation. For our project, we utilize a dataset collected by a group at the University of South Wales, which contains parameters, such as hand position, hand rotation, and finger bend, for 95 unique signs. For each input stream representing a sign, we predict which sign class this stream falls into. We begin by implementing baseline SVM and logistic regression models, which perform reasonably well on high-quality data. Lower quality data requires a more sophisticated approach, so we explore different methods in temporal classification, including long short-term memory architectures and sequential pattern mining methods.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/unpublished-2015-stanford-229/report.pdf">PDF</a> 
			<a class="research-action" href="/research/unpublished-2015-stanford-229/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-tex" data-lang="tex">@article{DBLP:journals/corr/CateDH17,
  author    = {Hardie Cate and
               Fahim Dalvi and
               Zeshan Hussain},
  title     = {Sign Language Recognition Using Temporal Classification},
  journal   = {CoRR},
  volume    = {abs/1701.01875},
  year      = {2017},
  url       = {http://arxiv.org/abs/1701.01875},
  archivePrefix = {arXiv},
  eprint    = {1701.01875},
  timestamp = {Wed, 07 Jun 2017 14:41:28 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/CateDH17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}</code></pre></div>
			</div>
		
	</div>
</div>


	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2015-stanford-231a/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Violet: Optimal Image Selection with Machine Learning</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Kai-Yuan Neo </i> </div>
		
		<div class="research-venue"><a href='http://cs231a.stanford.edu'>CS231a: Computer Vision, Stanford, 2015</a></div>
		<div class="research-abstract">	
			People often capture several photos of the same scene to produce the best image. Manually choosing the best image out of the candidates is a time consuming process. We propose an algorithm to automatically detect the optimal image out of a set of candidate images. This eliminates the need for people to spend time evaluating the quality of their images, and allows them to focus on enjoying the memories they have experienced.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/unpublished-2015-stanford-231a/report.pdf">PDF</a> 
			
			
			<a class="research-action" href="/research/unpublished-2015-stanford-231a/slides.pdf">Slides</a> 
			
		</div>
		
	</div>
</div>


	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2014-stanford-221/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>RTSift: Creating concise and meaningful review thread representations</b></div>
		<div class="research-authors"> <i> Kevin Chavez, <span class='highlight-author'>Fahim Dalvi</span> </i> </div>
		
		<div class="research-venue"><a href='http://cs221.stanford.edu'>CS221: Artificial Intelligence, Stanford, 2014</a></div>
		<div class="research-abstract">	
			We aim to produce a representation of review threads that is concise, interpretable, and preserves much of the meaning of the full text. Further, the representa- tion should be useful for various applications such as summarization, topic modeling, and star-rating pre- diction. This task can be modelled as a feature se- lection problem which consists of two stages: gen- erating feature proposals and filtering/ranking these features. The first stage automatically produces a large set of human-interpretable candidate features, while the second stage reduces that set to achieve a more concise representation.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/unpublished-2014-stanford-221/report.pdf">PDF</a> 
			<a class="research-action" href="/research/unpublished-2014-stanford-221/poster.pdf">Poster</a> 
			
			
			
		</div>
		
	</div>
</div>


	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2013-met-qcri/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Multi-User Backend for Meeting Translation</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Francisco Guzman </i> </div>
		
		<div class="research-venue"><a href='http://qcri.org.qa'>Qatar Computing Research Institute, 2013</a></div>
		<div class="research-abstract">	
			The aim of the Meeting translation project is to provide a platform for multi-lingual meetings. In order for the system to work efficiently, a robust backend is required to augment the automatic recognition and translation services. The existing backend was a very simple proof-of-concept that supported a single user only. The goal of this project was to develop a backend that could support the realtime needs of this project. The backend was also required to support multiple users and meetings simultaneously. Another important aspect of the project was to test the robustness and efficiency of the system. Hence, a statistics collection system was also required that could give us enough information about the different processes in the pipeline to analyze and pin-point the deficiencies in the system. Upon completion, a fully working system was built that could support multiple users and meetings. The system integrated well with the translation and transcription services already available. The statistics collection system was also built and the results from the system were used to analyze the bottlenecks in the processing. The system was tested in both the English and the Arabic language.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/unpublished-2013-met-qcri/report.pdf">PDF</a> 
			<a class="research-action" href="/research/unpublished-2013-met-qcri/poster.pdf">Poster</a> 
			
			
			
		</div>
		
	</div>
</div>


	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2013-i-want-my-mommy/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>I want my Mommy</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Syed Hashim Moosavi, Saquib Razak </i> </div>
		
		<div class="research-venue"><a href='https://issuu.com/carnegiemellonqatar/docs/mom_digest_2013_print/1'>Carnegie Mellon University Qatar, 2013</a></div>
		<div class="research-abstract">	
			“I want my Mommy” is a research project that aims to use wireless technologies such as Bluetooth and Wi-Fi to quickly locate people in a large crowd, subsequently reducing the number of lost people. In several crowded areas such as Makkah and Disneyland, people getting separated (specially children and elderly) from their families is a huge problem. This is currently handled manually by making announcements or giving people tags with information written on them. Unfortunately, these solutions do not work in highly crowded areas, both because of the number of people entering the location, and because of the size of these places. We plan to devise an algorithm using commonly existing wireless technologies to reduce the number of lost people by categorizing the crowd into groups without any barrier-to-entry.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			
			<a class="research-action" href="/research/unpublished-2013-i-want-my-mommy/poster.pdf">Poster</a> 
			
			
			
		</div>
		
	</div>
</div>


	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2012-riss/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Airboats Data Visualizer</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Balajee Kannan, Paul Scerri </i> </div>
		
		<div class="research-venue"><a href='https://riss.ri.cmu.edu/research_showcase/2012-posters/'>Robotics Institute, Carnegie Mellon, 2012</a></div>
		<div class="research-abstract">	
			Small, autonomous watercraft are an ideal approach to a number of applications in flood mitigation and response, environmental sampling, and numerous other applications. Relative to other types of vehicles, watercraft are inexpensive, simple, robust and reliable.  The vision is to have large numbers of very inexpensive airboats provide situational awareness and deliver critical emergency supplies to victims, as well as low cost tools for environmental protection and monitoring. My role in this project was to create a visual interface to analyze and understand the data collected by the boats.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			
			<a class="research-action" href="/research/unpublished-2012-riss/poster.pdf">Poster</a> 
			
			
			
		</div>
		
	</div>
</div>


	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2012-malware-inc/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Malware Inc - Web Browsers</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Baljit Singh, Thierry Sans </i> </div>
		
		<div class="research-venue"><a href='https://issuu.com/carnegiemellonqatar/docs/mom_digest_2012?backgroundColor=%2523222222'>Carnegie Mellon University Qatar, 2012</a></div>
		<div class="research-abstract">	
			Malware Inc. is a project that aims to study the development of Malware on various platforms, such as Web browsers, Social networks and Web engines. I chose Mozilla Firefox as my research platform, as it is the second most widely used web browser today. Having a wide audience adds to the importance of this research project, because the security of a higher number of people is at stake. My role in this research project was to study Firefox extensions, small pieces of code that help in enhancing the browsing experience. These extensions, although very helpful, have the potential to be used for malicious purposes.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/unpublished-2012-malware-inc/report.pdf">PDF</a> 
			<a class="research-action" href="/research/unpublished-2012-malware-inc/poster.pdf">Poster</a> 
			
			<a class="research-action" href="/research/unpublished-2012-malware-inc/slides.pdf">Slides</a> 
			
		</div>
		
	</div>
</div>


	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2011-hala-interface/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Integrating Natural Gestures in Touch Interfaces</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Ameer Abdulsalam, Majd Sakr </i> </div>
		
		<div class="research-venue"><a href='https://issuu.com/carnegiemellonqatar/docs/mom_digest_allpages?backgroundColor=%2523222222'>Carnegie Mellon University Qatar, 2011</a></div>
		<div class="research-abstract">	
			This work aims to explore the role of Natural Gestures in daily interaction with computer systems, in particular, their use in the navigation of touchscreen interfaces. Gestures provide a way for users to navigate an interface through intuitive on­screen touch motions and are leading to a shift from traditional point and click interactions to a more natural and physical way of interaction. Given the increased popularity of public touchscreen kiosks in various settings such as airports, hospitals and company lobbies, we designed and built a test­bed platform for exploring touchscreen interface design for users of mixed lingual and cultural backgrounds. Inspired by the increasing prominence of gestures in commercial touchscreen devices, our aim was to explore the effects of language and culture on gestures, including the impact of various aspects such as screen size on the usability and practicality of these gestures. We implemented a few of these gestures into our interface, such as natural scrolling, which enables the user to flick their fingers across the screen in order to browse through a list of items. As part of future work, we seek to implement additional gestures into the interface such as screen swiping and to deploy this system on a kiosk at Carnegie Mellon Qatar&#39;s campus for the purpose of collecting logs and running experiments. Through these experiments we seek to learn more about the interaction of users with the interface, their preferences and navigation performance, while considering the roles of language and culture in this region.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			
			<a class="research-action" href="/research/unpublished-2011-hala-interface/poster.pdf">Poster</a> 
			
			
			
		</div>
		
	</div>
</div>

</p>

</div>

  

</article>



      </main>

      <footer id='footer' class='footer'>
        <div class='container sep-before'>
          <section class='widget widget-social_menu sep-after'><nav aria-label='Social Menu'>
    <ul><li>
        <a href='https://github.com/fdalvi' target='_blank' rel='noopener'>
          <span class='screen-reader-text'>Open Github account in new tab</span><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/>
  
</svg>
</a>
      </li><li>
        <a href='mailto:dalvifahim&#43;website@gmail.com' target='_blank' rel='noopener'>
          <span class='screen-reader-text'>Contact via Email</span><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"/>
  <polyline points="22,6 12,13 2,6"/>
  
</svg>
</a>
      </li><li>
        <a href='https://linkedin.com/in/fdalvi' target='_blank' rel='noopener'>
          <span class='screen-reader-text'>Open Linkedin account in new tab</span><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"/>
  <rect x="2" y="9" width="4" height="12"/>
  <circle cx="4" cy="4" r="2"/>
  
</svg>
</a>
      </li><li>
        <a href='https://scholar.google.com/citations?user=uQGCv10AAAAJ' target='_blank' rel='noopener'>
          <span class='screen-reader-text'>Open Google_scholar account in new tab</span><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M21.328 2.002v9.2M8.695 7.85c.014-.787-.11-2.236.28-2.89.623-1.045.856-1.39 1.797-1.989 1.953-.988 4.296.692 4.296.692.803.564 1.672 2.1 1.672 2.1l1.368-1.824-5.444-1.754-3.515 1.34L6.08 7.681m9.109 3.42s.65-.633 1.168-1.085c.461-.402.516-.714.6-.914.18-.426.268-.909.268-1.446 0-.7-.131-1.274-.388-1.735-.031-.053 0 0-.097-.157l4.588-3.762H10.32L3.672 7.85l5.023-.024c.23 1.237.619 1.575 1.019 2.222.744.719 1.13 1.194 2.215 1.194.254 0 2.6-.057 2.842-.09 0 0 .546 1.199-.133 1.71-.41.31.576 1.304.576 1.304s-5.577.831-6.523 1.427a4.13 4.13 0 0 0-1.306 1.277 3.034 3.034 0 0 0-.493 1.665c0 .502.106.955.32 1.357.214.403.493.733.84.99.345.258.744.473 1.194.649.45.174.896.297 1.342.367a8.348 8.348 0 0 0 3.41-.166 7.754 7.754 0 0 0 1.964-.807 4.28 4.28 0 0 0 1.49-1.443c.38-.609.57-1.292.57-2.049 0-.574-.116-1.096-.347-1.57a3.755 3.755 0 0 0-.847-1.164c-.335-.302-2.19-1.837-2.19-1.837"/>
  
</svg>
</a>
      </li></ul>
  </nav>
</section>
          <div class='copyright'>
  <p> &copy; 2017-2018 Fahim Dalvi. Powered by <a href="https://gohugo.io" target="_blank">Hugo</a> and <a href="https://github.com/MunifTanjim/minimo" target="_blank">Minimo</a></p>
</div>

        </div>
      </footer>

    </div>
  </div><script>window.__public_path__='\/assets\/js\/'</script>

<script src='/assets/js/main.4d3afaaf.js'></script>
</body>

</html>


<!DOCTYPE html>
<html lang='en' dir='auto'><head>
  <meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='description' content='Recent research publications'>
<meta name='theme-color' content='#ffcd00'>

<meta property='og:title' content='Research • Fahim Dalvi'>
<meta property='og:description' content='Recent research publications'>
<meta property='og:url' content='https://fdalvi.github.io/research/'>
<meta property='og:site_name' content='Fahim Dalvi'>
<meta property='og:type' content='article'><meta property='article:section' content='page'><meta name='twitter:card' content='summary'>

<meta name="generator" content="Hugo 0.118.2">

  <title>Research • Fahim Dalvi</title>
  <link rel='canonical' href='https://fdalvi.github.io/research/'>
  
  
  
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png?v=almNBqOxQo">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png?v=almNBqOxQo">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png?v=almNBqOxQo">
<link rel="manifest" href="/site.webmanifest?v=almNBqOxQo">
<link rel="mask-icon" href="/safari-pinned-tab.svg?v=almNBqOxQo" color="#444444">
<link rel="shortcut icon" href="/favicon.ico?v=almNBqOxQo">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

<link rel='stylesheet' href='/assets/css/main.ab98e12b.css'><link rel='stylesheet' href='/css/custom.css'><style>
:root{--color-accent:#ffcd00;}
</style>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-19417017-2', 'auto');
	
	ga('send', 'pageview');
}
</script>
  

</head>
<body class='page type-page'>

  <div class='site'><a class='screen-reader-text' href='#content'>Skip to Content</a><div class='main'><nav id='main-menu' class='menu main-menu' aria-label='Main Menu'>
  <div class='container'>
    
    <ul><li class='item'>
        <a href='/'>Home</a>
      </li><li class='item'>
        <a href='/projects/'>Projects</a>
      </li><li class='item current'>
        <a aria-current='page' href='/research/'>Research</a>
      </li><li class='item'>
        <a href='/teaching/'>Teaching</a>
      </li><li class='item'>
        <a href='/blog/'>Blog</a>
      </li><li class='item'>
        <a href='/fdalvi-resume.pdf'>Resume</a>
      </li></ul>
  </div>
</nav><div class='header-widgets'>
        <div class='container'></div>
      </div>

      <header id='header' class='header site-header'>
        <div class='container sep-after'>
          <div class='header-info'><p class='site-title title'>Fahim Dalvi</p><p class='desc site-desc'>Software Engineer | Deep Learning Researcher | Mentor</p>
          </div>
        </div>
      </header>

      <main id='content'>


<article lang='en' class='entry'>
  <header class='header entry-header'>
  <div class='container sep-after'>
    <div class='header-info'>
      <h1 class='title'>Research</h1>
      
<p class='desc'>Recent research publications</p>


    </div>
    

  </div>
</header>

  
  

  <div class='container entry-content'>
  <h3 id="2024">2024</h3>

	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2024-03-EACL-LLMeBench/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Maram Hasanain, Sabri Boughorbel, Basel Mousi, Samir Abdaljalil, Nizi Nazar, Ahmed Abdelali, Shammur Absar Chowdhury, Hamdy Mubarak, Ahmed Ali, Majd Hawasly, Nadir Durrani, Firoj Alam </i> </div>
		
		<div class="research-venue"><a href='https://2024.eacl.org'>The 18th Conference of the European Chapter of the Association for Computational Linguistics</a></div>
		<div class="research-abstract">	
			The recent development and success of Large Language Models (LLMs) necessitate an evaluation of their performance across diverse NLP tasks in different languages. Although several frameworks have been developed and made publicly available, their customization capabilities for specific tasks and datasets are often complex for different users. In this study, we introduce the LLMeBench framework, which can be seamlessly customized to evaluate LLMs for any NLP task, regardless of language. The framework features generic dataset loaders, several model providers, and pre-implements most standard evaluation metrics. It supports in-context learning with zero- and few-shot settings. A specific dataset and task can be evaluated for a given LLM in less than 20 lines of code while allowing full flexibility to extend the framework for custom datasets, models, or tasks. The framework has been tested on 31 unique NLP tasks using 53 publicly available datasets within 90 experimental setups, involving approximately 296K data points. We open-sourced LLMeBench for the community (https://github.com/qcri/LLMeBench/) and a video demonstrating the framework is available online (https://youtu.be/9cC2m_abk3A).
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2024-03-EACL-LLMeBench/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2024-03-EACL-LLMeBench/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			<a class="research-action" href="https://github.com/qcri/LLMeBench/">Code</a> 
			
			<a class="research-action" href="https://youtu.be/9cC2m_abk3A">Video</a> 
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{dalvi-etal-2024-llmebench,
</span></span><span style="display:flex;"><span>  title = &#34;{LLM}e{B}ench: A Flexible Framework for Accelerating {LLM}s Benchmarking&#34;,
</span></span><span style="display:flex;"><span>  author = &#34;Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>    Hasanain, Maram  and
</span></span><span style="display:flex;"><span>    Boughorbel, Sabri  and
</span></span><span style="display:flex;"><span>    Mousi, Basel  and
</span></span><span style="display:flex;"><span>    Abdaljalil, Samir  and
</span></span><span style="display:flex;"><span>    Nazar, Nizi  and
</span></span><span style="display:flex;"><span>    Abdelali, Ahmed  and
</span></span><span style="display:flex;"><span>    Chowdhury, Shammur Absar  and
</span></span><span style="display:flex;"><span>    Mubarak, Hamdy  and
</span></span><span style="display:flex;"><span>    Ali, Ahmed and
</span></span><span style="display:flex;"><span>    Hawasly, Majd and
</span></span><span style="display:flex;"><span>    Durrani, Nadir and
</span></span><span style="display:flex;"><span>    Alam, Firoj&#34;,
</span></span><span style="display:flex;"><span>  editor = &#34;Aletras, Nikolaos  and
</span></span><span style="display:flex;"><span>    De Clercq, Orphee&#34;,
</span></span><span style="display:flex;"><span>  booktitle = &#34;Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations&#34;,
</span></span><span style="display:flex;"><span>  month = mar,
</span></span><span style="display:flex;"><span>  year = &#34;2024&#34;,
</span></span><span style="display:flex;"><span>  address = &#34;St. Julians, Malta&#34;,
</span></span><span style="display:flex;"><span>  publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>  url = &#34;https://aclanthology.org/2024.eacl-demo.23&#34;,
</span></span><span style="display:flex;"><span>  pages = &#34;214--222&#34;,
</span></span><span style="display:flex;"><span>  abstract = &#34;The recent development and success of Large Language Models (LLMs) necessitate an evaluation of their performance across diverse NLP tasks in different languages. Although several frameworks have been developed and made publicly available, their customization capabilities for specific tasks and datasets are often complex for different users. In this study, we introduce the LLMeBench framework, which can be seamlessly customized to evaluate LLMs for any NLP task, regardless of language. The framework features generic dataset loaders, several model providers, and pre-implements most standard evaluation metrics. It supports in-context learning with zero- and few-shot settings. A specific dataset and task can be evaluated for a given LLM in less than 20 lines of code while allowing full flexibility to extend the framework for custom datasets, models, or tasks. The framework has been tested on 31 unique NLP tasks using 53 publicly available datasets within 90 experimental setups, involving approximately 296K data points. We open-sourced LLMeBench for the community (https://github.com/qcri/LLMeBench/) and a video demonstrating the framework is available online (https://youtu.be/9cC2m{<span style="color:#66d9ef">\_</span>}abk3A).&#34;,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2024-03-EACL-scaling-discovery-latent-concepts/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Scaling up Discovery of Latent Concepts in Deep NLP Models</b></div>
		<div class="research-authors"> <i> Majd Hawasly, <span class='highlight-author'>Fahim Dalvi</span>, Nadir Durrani </i> </div>
		
		<div class="research-venue"><a href='https://2024.eacl.org'>The 18th Conference of the European Chapter of the Association for Computational Linguistics</a></div>
		<div class="research-abstract">	
			Despite the revolution caused by deep NLP models, they remain black boxes, necessitating research to understand their decision-making processes. A recent work by Dalvi et al. (2022) carried out representation analysis through the lens of clustering latent spaces within pre-trained models (PLMs), but that approach is limited to small scale due to the high cost of running Agglomerative hierarchical clustering. This paper studies clustering algorithms in order to scale the discovery of encoded concepts in PLM representations to larger datasets and models. We propose metrics for assessing the quality of discovered latent concepts and use them to compare the studied clustering algorithms. We found that K-Means-based concept discovery significantly enhances efficiency while maintaining the quality of the obtained concepts. Furthermore, we demonstrate the practicality of this newfound efficiency by scaling latent concept discovery to LLMs and phrasal concepts.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2024-03-EACL-scaling-discovery-latent-concepts/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			<a class="research-action" href="https://aclanthology.org/attachments/2024.eacl-long.48.software.zip">Code</a> 
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{hawasly-etal-2024-scaling,
</span></span><span style="display:flex;"><span>    title = &#34;Scaling up Discovery of Latent Concepts in Deep {NLP} Models&#34;,
</span></span><span style="display:flex;"><span>    author = &#34;Hawasly, Majd  and
</span></span><span style="display:flex;"><span>      Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>      Durrani, Nadir&#34;,
</span></span><span style="display:flex;"><span>    editor = &#34;Graham, Yvette  and
</span></span><span style="display:flex;"><span>      Purver, Matthew&#34;,
</span></span><span style="display:flex;"><span>    booktitle = &#34;Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)&#34;,
</span></span><span style="display:flex;"><span>    month = mar,
</span></span><span style="display:flex;"><span>    year = &#34;2024&#34;,
</span></span><span style="display:flex;"><span>    address = &#34;St. Julian{&#39;}s, Malta&#34;,
</span></span><span style="display:flex;"><span>    publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    url = &#34;https://aclanthology.org/2024.eacl-long.48&#34;,
</span></span><span style="display:flex;"><span>    pages = &#34;793--806&#34;,
</span></span><span style="display:flex;"><span>    abstract = &#34;Despite the revolution caused by deep NLP models, they remain black boxes, necessitating research to understand their decision-making processes. A recent work by Dalvi et al. (2022) carried out representation analysis through the lens of clustering latent spaces within pre-trained models (PLMs), but that approach is limited to small scale due to the high cost of running Agglomerative hierarchical clustering. This paper studies clustering algorithms in order to scale the discovery of encoded concepts in PLM representations to larger datasets and models. We propose metrics for assessing the quality of discovered latent concepts and use them to compare the studied clustering algorithms. We found that K-Means-based concept discovery significantly enhances efficiency while maintaining the quality of the obtained concepts. Furthermore, we demonstrate the practicality of this newfound efficiency by scaling latent concept discovery to LLMs and phrasal concepts.&#34;,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2024-03-EACL-LAraBench/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>LAraBench: Benchmarking Arabic AI with Large Language Models</b></div>
		<div class="research-authors"> <i> Ahmed Abdelali, Hamdy Mubarak, Shammur Chowdhury, Maram Hasanain, Basel Mousi, Sabri Boughorbel, Samir Abdaljalil, Yassine El Kheir, Daniel Izham, <span class='highlight-author'>Fahim Dalvi</span>, Majd Hawasly, Nizi Nazar, Youssef Elshahawy, Ahmed Ali, Nadir Durrani, Natasa Milic-Frayling, Firoj Alam </i> </div>
		
		<div class="research-venue"><a href='https://2024.eacl.org'>The 18th Conference of the European Chapter of the Association for Computational Linguistics</a></div>
		<div class="research-abstract">	
			Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to tackle 33 distinct tasks across 61 publicly available datasets. This involved 98 experimental setups, encompassing ~296K data points, ~46 hours of speech, and 30 sentences for Text-to-Speech (TTS). This effort resulted in 330&#43; sets of experiments. Our analysis focused on measuring the performance gap between SOTA models and LLMs. The overarching trend observed was that SOTA models generally outperformed LLMs in zero-shot learning, with a few exceptions. Notably, larger computational models with few-shot learning techniques managed to reduce these performance gaps. Our findings provide valuable insights into the applicability of LLMs for Arabic NLP and speech processing tasks.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2024-03-EACL-LAraBench/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2024-03-EACL-LAraBench/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			<a class="research-action" href="https://github.com/qcri/LLMeBench/">Code</a> 
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{abdelali-etal-2024-larabench,
</span></span><span style="display:flex;"><span>    title = &#34;{LA}ra{B}ench: Benchmarking {A}rabic {AI} with Large Language Models&#34;,
</span></span><span style="display:flex;"><span>    author = &#34;Abdelali, Ahmed  and
</span></span><span style="display:flex;"><span>      Mubarak, Hamdy  and
</span></span><span style="display:flex;"><span>      Chowdhury, Shammur  and
</span></span><span style="display:flex;"><span>      Hasanain, Maram  and
</span></span><span style="display:flex;"><span>      Mousi, Basel  and
</span></span><span style="display:flex;"><span>      Boughorbel, Sabri  and
</span></span><span style="display:flex;"><span>      Abdaljalil, Samir  and
</span></span><span style="display:flex;"><span>      El Kheir, Yassine  and
</span></span><span style="display:flex;"><span>      Izham, Daniel  and
</span></span><span style="display:flex;"><span>      Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>      Hawasly, Majd  and
</span></span><span style="display:flex;"><span>      Nazar, Nizi  and
</span></span><span style="display:flex;"><span>      Elshahawy, Youssef  and
</span></span><span style="display:flex;"><span>      Ali, Ahmed  and
</span></span><span style="display:flex;"><span>      Durrani, Nadir  and
</span></span><span style="display:flex;"><span>      Milic-Frayling, Natasa  and
</span></span><span style="display:flex;"><span>      Alam, Firoj&#34;,
</span></span><span style="display:flex;"><span>    editor = &#34;Graham, Yvette  and
</span></span><span style="display:flex;"><span>      Purver, Matthew&#34;,
</span></span><span style="display:flex;"><span>    booktitle = &#34;Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)&#34;,
</span></span><span style="display:flex;"><span>    month = mar,
</span></span><span style="display:flex;"><span>    year = &#34;2024&#34;,
</span></span><span style="display:flex;"><span>    address = &#34;St. Julian{&#39;}s, Malta&#34;,
</span></span><span style="display:flex;"><span>    publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    url = &#34;https://aclanthology.org/2024.eacl-long.30&#34;,
</span></span><span style="display:flex;"><span>    pages = &#34;487--520&#34;,
</span></span><span style="display:flex;"><span>    abstract = &#34;Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to tackle 33 distinct tasks across 61 publicly available datasets. This involved 98 experimental setups, encompassing {<span style="color:#66d9ef">\textasciitilde</span>}296K data points, {<span style="color:#66d9ef">\textasciitilde</span>}46 hours of speech, and 30 sentences for Text-to-Speech (TTS). This effort resulted in 330+ sets of experiments. Our analysis focused on measuring the performance gap between SOTA models and LLMs. The overarching trend observed was that SOTA models generally outperformed LLMs in zero-shot learning, with a few exceptions. Notably, larger computational models with few-shot learning techniques managed to reduce these performance gaps. Our findings provide valuable insights into the applicability of LLMs for Arabic NLP and speech processing tasks.&#34;,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


<h3 id="2023">2023</h3>

	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2023-12-NeurIPS-evaluating-neuron-interpretation/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Evaluating Neuron Interpretation Methods of NLP Models</b></div>
		<div class="research-authors"> <i> Yimin Fan, <span class='highlight-author'>Fahim Dalvi</span>, Nadir Durrani, Hassan Sajjad </i> </div>
		
		<div class="research-venue"><a href='https://neurips.cc/Conferences/2023'>The Thirty-seventh Annual Conference on Neural Information Processing Systems</a></div>
		<div class="research-abstract">	
			Neuron interpretation offers valuable insights into how knowledge is structured within a deep neural network model. While a number of neuron interpretation methods have been proposed in the literature, the field lacks a comprehensive comparison among these methods. This gap hampers progress due to the absence of standardized metrics and benchmarks. The commonly used evaluation metric has limitations, and creating ground truth annotations for neurons is impractical. Addressing these challenges, we propose an evaluation framework based on voting theory. Our hypothesis posits that neurons consistently identified by different methods carry more significant information. We rigorously assess our framework across a diverse array of neuron interpretation methods. Notable findings include: i) despite the theoretical differences among the methods, neuron ranking methods share over 60% of their rankings when identifying salient neurons, ii) the neuron interpretation methods are most sensitive to the last layer representations, iii) Probeless neuron ranking emerges as the most consistent method.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2023-12-NeurIPS-evaluating-neuron-interpretation/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2023-12-NeurIPS-evaluating-neuron-interpretation/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			<a class="research-action" href="https://github.com/fdalvi/neuron-comparative-analysis">Code</a> 
			
			<a class="research-action" href="https://neurips.cc/virtual/2023/poster/71278">Video</a> 
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{NEURIPS2023_eef6cb60,
</span></span><span style="display:flex;"><span> author = {Fan, Yimin and Dalvi, Fahim and Durrani, Nadir and Sajjad, Hassan},
</span></span><span style="display:flex;"><span> booktitle = {Advances in Neural Information Processing Systems},
</span></span><span style="display:flex;"><span> editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
</span></span><span style="display:flex;"><span> pages = {75644--75668},
</span></span><span style="display:flex;"><span> publisher = {Curran Associates, Inc.},
</span></span><span style="display:flex;"><span> title = {Evaluating Neuron Interpretation Methods of NLP Models},
</span></span><span style="display:flex;"><span> url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/eef6cb60fd59b32d35718e176b4b08d6-Paper-Conference.pdf},
</span></span><span style="display:flex;"><span> volume = {36},
</span></span><span style="display:flex;"><span> year = {2023}
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2023-12-JMLR-discovering-neurons/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Discovering Salient Neurons in Deep NLP models</b></div>
		<div class="research-authors"> <i> Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span>, Hassan Sajjad </i> </div>
		
		<div class="research-venue"><a href='https://jmlr.org'>Journal of Machine Learning Research</a></div>
		<div class="research-abstract">	
			While a lot of work has been done in understanding representations learned within deep NLP models and what knowledge they capture, work done towards analyzing individual neurons is relatively sparse. We present a technique called Linguistic Correlation Analysis to extract salient neurons in the model, with respect to any extrinsic property, with the goal of understanding how such knowledge is preserved within neurons. We carry out a fine-grained analysis to answer the following questions: (i) can we identify subsets of neurons in the network that learn a specific linguistic property? (ii) is a certain linguistic phenomenon in a given model localized (encoded in few individual neurons) or distributed across many neurons? (iii) how redundantly is the information preserved? (iv) how does fine-tuning pre-trained models towards downstream NLP tasks impact the learned linguistic knowledge? (v) how do models vary in learning different linguistic properties? Our data-driven, quantitative analysis illuminates interesting findings: (i) we found small subsets of neurons that can predict different linguistic tasks; (ii) neurons capturing basic lexical information, such as suffixation, are localized in the lowermost layers; (iii) neurons learning complex concepts, such as syntactic role, are predominantly found in middle and higher layers; (iv) salient linguistic neurons are relocated from higher to lower layers during transfer learning, as the network preserves the higher layers for task-specific information; (v) we found interesting differences across pre-trained models regarding how linguistic information is preserved within them; and (vi) we found that concepts exhibit similar neuron distribution across different languages in the multilingual transformer models. Our code is publicly available as part of the NeuroX toolkit (Dalvi et al., 2023).
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2023-12-JMLR-discovering-neurons/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			<a class="research-action" href="https://neurox.qcri.org/">Code</a> 
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@article{JMLR:v24:23-0074,
</span></span><span style="display:flex;"><span>  author  = {Nadir Durrani and Fahim Dalvi and Hassan Sajjad},
</span></span><span style="display:flex;"><span>  title   = {Discovering Salient Neurons in deep NLP models},
</span></span><span style="display:flex;"><span>  journal = {Journal of Machine Learning Research},
</span></span><span style="display:flex;"><span>  year    = {2023},
</span></span><span style="display:flex;"><span>  volume  = {24},
</span></span><span style="display:flex;"><span>  number  = {362},
</span></span><span style="display:flex;"><span>  pages   = {1--40},
</span></span><span style="display:flex;"><span>  url     = {http://jmlr.org/papers/v24/23-0074.html}
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2023-12-EMNLP-LLMs-interpretation/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Can LLMs Facilitate Interpretation of Pre-trained Language Models?</b></div>
		<div class="research-authors"> <i> Basel Mousi, Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span> </i> </div>
		
		<div class="research-venue"><a href='https://2023.emnlp.org'>The 2023 Conference on Empirical Methods in Natural Language Processing</a></div>
		<div class="research-abstract">	
			Work done to uncover the knowledge encoded within pre-trained language models rely on annotated corpora or human-in-the-loop methods. However, these approaches are limited in terms of scalability and the scope of interpretation. We propose using a large language model, ChatGPT, as an annotator to enable fine-grained interpretation analysis of pre-trained language models. We discover latent concepts within pre-trained language models by applying agglomerative hierarchical clustering over contextualized representations and then annotate these concepts using ChatGPT. Our findings demonstrate that ChatGPT produces accurate and semantically richer annotations compared to human-annotated concepts. Additionally, we showcase how GPT-based annotations empower interpretation analysis methodologies of which we demonstrate two: probing frameworks and neuron interpretation. To facilitate further exploration and experimentation in the field, we make available a substantial ConceptNet dataset (TCN) comprising 39,000 annotated concepts.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2023-12-EMNLP-LLMs-interpretation/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			<a class="research-action" href="https://neurox.qcri.org/projects/transformers-concept-net/">Code</a> 
			
			<a class="research-action" href="https://aclanthology.org/2023.emnlp-main.196.mp4">Video</a> 
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{mousi-etal-2023-llms,
</span></span><span style="display:flex;"><span>    title = &#34;Can {LLM}s Facilitate Interpretation of Pre-trained Language Models?&#34;,
</span></span><span style="display:flex;"><span>    author = &#34;Mousi, Basel  and
</span></span><span style="display:flex;"><span>      Durrani, Nadir  and
</span></span><span style="display:flex;"><span>      Dalvi, Fahim&#34;,
</span></span><span style="display:flex;"><span>    editor = &#34;Bouamor, Houda  and
</span></span><span style="display:flex;"><span>      Pino, Juan  and
</span></span><span style="display:flex;"><span>      Bali, Kalika&#34;,
</span></span><span style="display:flex;"><span>    booktitle = &#34;Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing&#34;,
</span></span><span style="display:flex;"><span>    month = dec,
</span></span><span style="display:flex;"><span>    year = &#34;2023&#34;,
</span></span><span style="display:flex;"><span>    address = &#34;Singapore&#34;,
</span></span><span style="display:flex;"><span>    publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    url = &#34;https://aclanthology.org/2023.emnlp-main.196&#34;,
</span></span><span style="display:flex;"><span>    doi = &#34;10.18653/v1/2023.emnlp-main.196&#34;,
</span></span><span style="display:flex;"><span>    pages = &#34;3248--3268&#34;,
</span></span><span style="display:flex;"><span>    abstract = &#34;Work done to uncover the knowledge encoded within pre-trained language models rely on annotated corpora or human-in-the-loop methods. However, these approaches are limited in terms of scalability and the scope of interpretation. We propose using a large language model, ChatGPT, as an annotator to enable fine-grained interpretation analysis of pre-trained language models. We discover latent concepts within pre-trained language models by applying agglomerative hierarchical clustering over contextualized representations and then annotate these concepts using ChatGPT. Our findings demonstrate that ChatGPT produces accurate and semantically richer annotations compared to human-annotated concepts. Additionally, we showcase how GPT-based annotations empower interpretation analysis methodologies of which we demonstrate two: probing frameworks and neuron interpretation. To facilitate further exploration and experimentation in the field, we make available a substantial ConceptNet dataset (TCN) comprising 39,000 annotated concepts.&#34;,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2023-07-ACL-NeuroX-demo/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>NeuroX Library for Neuron Analysis of Deep NLP Models</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Hassan Sajjad, Nadir Durrani </i> </div>
		
		<div class="research-venue"><a href='https://2023.aclweb.org'>The 61st Annual Meeting of the Association for Computational Linguistics</a></div>
		<div class="research-abstract">	
			Neuron analysis provides insights into how knowledge is structured in representations and discovers the role of neurons in the network. In addition to developing an understanding of our models, neuron analysis enables various applications such as debiasing, domain adaptation and architectural search. We present NeuroX, a comprehensive open-source toolkit to conduct neuron analysis of natural language processing models. It implements various interpretation methods under a unified API, and provides a framework for data processing and evaluation, thus making it easier for researchers and practitioners to perform neuron analysis. The Python toolkit is available at https://www.github.com/fdalvi/NeuroX.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2023-07-ACL-NeuroX-demo/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2023-07-ACL-NeuroX-demo/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			<a class="research-action" href="https://www.github.com/fdalvi/NeuroX">Code</a> 
			
			<a class="research-action" href="https://www.youtube.com/watch?v=PRoiGMex4_U">Video</a> 
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{dalvi-etal-2023-neurox,
</span></span><span style="display:flex;"><span>    title = &#34;{N}euro{X} Library for Neuron Analysis of Deep {NLP} Models&#34;,
</span></span><span style="display:flex;"><span>    author = &#34;Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>      Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>      Durrani, Nadir&#34;,
</span></span><span style="display:flex;"><span>    booktitle = &#34;Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)&#34;,
</span></span><span style="display:flex;"><span>    month = jul,
</span></span><span style="display:flex;"><span>    year = &#34;2023&#34;,
</span></span><span style="display:flex;"><span>    address = &#34;Toronto, Canada&#34;,
</span></span><span style="display:flex;"><span>    publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    url = &#34;https://aclanthology.org/2023.acl-demo.21&#34;,
</span></span><span style="display:flex;"><span>    doi = &#34;10.18653/v1/2023.acl-demo.21&#34;,
</span></span><span style="display:flex;"><span>    pages = &#34;226--234&#34;,
</span></span><span style="display:flex;"><span>    abstract = &#34;Neuron analysis provides insights into how knowledge is structured in representations and discovers the role of neurons in the network. In addition to developing an understanding of our models, neuron analysis enables various applications such as debiasing, domain adaptation and architectural search. We present NeuroX, a comprehensive open-source toolkit to conduct neuron analysis of natural language processing models. It implements various interpretation methods under a unified API, and provides a framework for data processing and evaluation, thus making it easier for researchers and practitioners to perform neuron analysis. The Python toolkit is available at https://www.github.com/fdalvi/NeuroX.Demo Video available at: https://youtu.be/mLhs2YMx4u8&#34;,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2023-05-EACL-nxplain/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>NxPlain: A Web-based Tool for Discovery of Latent Concepts</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Nadir Durrani, Hassan Sajjad, Tamim Jaban, Mus’ab Husaini, Ummar Abbas </i> </div>
		
		<div class="research-venue"><a href='https://2023.eacl.org'>The 17th Conference of the European Chapter of the Association for Computational Linguistics</a></div>
		<div class="research-abstract">	
			The proliferation of deep neural networks in various domains has seen an increased need for the interpretability of these models, especially in scenarios where fairness and trust are as important as model performance. A lot of independent work is being carried out to: i) analyze what linguistic and non-linguistic knowledge is learned within these models, and ii) highlight the salient parts of the input. We present NxPlain, a web-app that provides an explanation of a model{&#39;}s prediction using latent concepts. NxPlain discovers latent concepts learned in a deep NLP model, provides an interpretation of the knowledge learned in the model, and explains its predictions based on the used concepts. The application allows users to browse through the latent concepts in an intuitive order, letting them efficiently scan through the most salient concepts with a global corpus-level view and a local sentence-level view. Our tool is useful for debugging, unraveling model bias, and for highlighting spurious correlations in a model. A hosted demo is available here: https://nxplain.qcri.org
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2023-05-EACL-nxplain/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2023-05-EACL-nxplain/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			<a class="research-action" href="https://youtu.be/C2PiO4fI5dk">Video</a> 
			
			<a class="research-action" href="https://nxplain.qcri.org">Demo</a> 
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{dalvi-etal-2023-nxplain,
</span></span><span style="display:flex;"><span>  title = &#34;{N}x{P}lain: A Web-based Tool for Discovery of Latent Concepts&#34;,
</span></span><span style="display:flex;"><span>  author = &#34;Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>    Durrani, Nadir  and
</span></span><span style="display:flex;"><span>    Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>    Jaban, Tamim  and
</span></span><span style="display:flex;"><span>    Husaini, Mus{&#39;}ab  and
</span></span><span style="display:flex;"><span>    Abbas, Ummar&#34;,
</span></span><span style="display:flex;"><span>  booktitle = &#34;Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations&#34;,
</span></span><span style="display:flex;"><span>  month = may,
</span></span><span style="display:flex;"><span>  year = &#34;2023&#34;,
</span></span><span style="display:flex;"><span>  address = &#34;Dubrovnik, Croatia&#34;,
</span></span><span style="display:flex;"><span>  publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>  url = &#34;https://aclanthology.org/2023.eacl-demo.10&#34;,
</span></span><span style="display:flex;"><span>  doi = &#34;10.18653/v1/2023.eacl-demo.10&#34;,
</span></span><span style="display:flex;"><span>  pages = &#34;75--83&#34;,
</span></span><span style="display:flex;"><span>  abstract = &#34;The proliferation of deep neural networks in various domains has seen an increased need for the interpretability of these models, especially in scenarios where fairness and trust are as important as model performance. A lot of independent work is being carried out to: i) analyze what linguistic and non-linguistic knowledge is learned within these models, and ii) highlight the salient parts of the input. We present NxPlain, a web-app that provides an explanation of a model{&#39;}s prediction using latent concepts. NxPlain discovers latent concepts learned in a deep NLP model, provides an interpretation of the knowledge learned in the model, and explains its predictions based on the used concepts. The application allows users to browse through the latent concepts in an intuitive order, letting them efficiently scan through the most salient concepts with a global corpus-level view and a local sentence-level view. Our tool is useful for debugging, unraveling model bias, and for highlighting spurious correlations in a model. A hosted demo is available here: https://nxplain.qcri.org&#34;,
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2023-02-AAAI-conceptx-demo/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>ConceptX: A Framework for Latent Concept Analysis</b></div>
		<div class="research-authors"> <i> Firoj Alam, <span class='highlight-author'>Fahim Dalvi</span>, Nadir Durrani, Hassan Sajjad, Abdul Rafae Khan, Jia Xu </i> </div>
		
		<div class="research-venue"><a href='https://aaai-23.aaai.org'>The Thirty-Seventh AAAI Conference On Artificial Intelligence (AAAI-23)</a></div>
		<div class="research-abstract">	
			The opacity of deep neural networks remains a challenge in deploying solutions where explanation is as important as precision. We present ConceptX, a human-in-the-loop framework for interpreting and annotating latent representational space in pre-trained Language Models (pLMs). We use an unsupervised method to discover concepts learned in these models and enable a graphical interface for humans to generate explanations for the concepts. To facilitate the process, we provide auto-annotations of the concepts (based on traditional linguistic ontologies). Such annotations enable development of a linguistic resource that directly represents latent concepts learned within deep NLP models. These include not just traditional linguistic concepts, but also task-specific or sensitive concepts (words grouped based on gender or religious connotation) that helps the annotators to mark bias in the model. The framework consists of two parts (i) concept discovery and (ii) annotation platform.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2023-02-AAAI-conceptx-demo/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@article{Alam_Dalvi_Durrani_Sajjad_Khan_Xu_2023, 
</span></span><span style="display:flex;"><span>    title={ConceptX: A Framework for Latent Concept Analysis},
</span></span><span style="display:flex;"><span>    volume={37},
</span></span><span style="display:flex;"><span>    url={https://ojs.aaai.org/index.php/AAAI/article/view/27057},
</span></span><span style="display:flex;"><span>    DOI={10.1609/aaai.v37i13.27057},
</span></span><span style="display:flex;"><span>    abstractNote={The opacity of deep neural networks remains a challenge in deploying solutions where explanation is as important as precision. We present ConceptX, a human-in-the-loop framework for interpreting and annotating latent representational space in pre-trained Language Models (pLMs). We use an unsupervised method to discover concepts learned in these models and enable a graphical interface for humans to generate explanations for the concepts. To facilitate the process, we provide auto-annotations of the concepts (based on traditional linguistic ontologies). Such annotations enable development of a linguistic resource that directly represents latent concepts learned within deep NLP models. These include not just traditional linguistic concepts, but also task-specific or sensitive concepts (words grouped based on gender or religious connotation) that helps the annotators to mark bias in the model. The framework consists of two parts (i) concept discovery and (ii) annotation platform.},
</span></span><span style="display:flex;"><span>    number={13},
</span></span><span style="display:flex;"><span>    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
</span></span><span style="display:flex;"><span>    author={Alam, Firoj and Dalvi, Fahim and Durrani, Nadir and Sajjad, Hassan and Khan, Abdul Rafae and Xu, Jia},
</span></span><span style="display:flex;"><span>    year={2023},
</span></span><span style="display:flex;"><span>    month={Sep.},
</span></span><span style="display:flex;"><span>    pages={16395-16397}
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2023-01-CSL-dropping-layers-transformers/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>On the effect of dropping layers of pre-trained transformer models</b></div>
		<div class="research-authors"> <i> Hassan Sajjad, <span class='highlight-author'>Fahim Dalvi</span>, Nadir Durrani, Preslav Nakov </i> </div>
		
		<div class="research-venue"><a href='https://www.sciencedirect.com/journal/computer-speech-and-language/vol/77/suppl/C'>Computer Speech & Language, Volume 77</a></div>
		<div class="research-abstract">	
			Transformer-based NLP models are trained using hundreds of millions or even billions of parameters, limiting their applicability in computationally constrained environments. While the number of parameters generally correlates with performance, it is not clear whether the entire network is required for a downstream task. Motivated by the recent work on pruning and distilling pre-trained models, we explore strategies to drop layers in pre-trained models, and observe the effect of pruning on downstream GLUE tasks. We were able to prune BERT, RoBERTa and XLNet models up to 40%, while maintaining up to 98% of their original performance. Additionally we show that our pruned models are on par with those built using knowledge distillation, both in terms of size and performance. Our experiments yield interesting observations such as: (i) the lower layers are most critical to maintain downstream task performance, (ii) some tasks such as paraphrase detection and sentence similarity are more robust to the dropping of layers, and (iii) models trained using different objective function exhibit different learning patterns and w.r.t the layer dropping.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2023-01-CSL-dropping-layers-transformers/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@article{SAJJAD2023101429,
</span></span><span style="display:flex;"><span>    title = {On the effect of dropping layers of pre-trained transformer models},
</span></span><span style="display:flex;"><span>    journal = {Computer Speech &amp; Language},
</span></span><span style="display:flex;"><span>    volume = {77},
</span></span><span style="display:flex;"><span>    pages = {101429},
</span></span><span style="display:flex;"><span>    year = {2023},
</span></span><span style="display:flex;"><span>    issn = {0885-2308},
</span></span><span style="display:flex;"><span>    doi = {https://doi.org/10.1016/j.csl.2022.101429},
</span></span><span style="display:flex;"><span>    url = {https://www.sciencedirect.com/science/article/pii/S0885230822000596},
</span></span><span style="display:flex;"><span>    author = {Hassan Sajjad and Fahim Dalvi and Nadir Durrani and Preslav Nakov},
</span></span><span style="display:flex;"><span>    keywords = {Pre-trained transformer models, Efficient transfer learning, Interpretation and analysis},
</span></span><span style="display:flex;"><span>    abstract = {Transformer-based NLP models are trained using hundreds of millions or even billions of parameters, limiting their applicability in computationally constrained environments. While the number of parameters generally correlates with performance, it is not clear whether the entire network is required for a downstream task. Motivated by the recent work on pruning and distilling pre-trained models, we explore strategies to drop layers in pre-trained models, and observe the effect of pruning on downstream GLUE tasks. We were able to prune BERT, RoBERTa and XLNet models up to 40<span style="color:#75715e">%, while maintaining up to 98% of their original performance. Additionally we show that our pruned models are on par with those built using knowledge distillation, both in terms of size and performance. Our experiments yield interesting observations such as: (i) the lower layers are most critical to maintain downstream task performance, (ii) some tasks such as paraphrase detection and sentence similarity are more robust to the dropping of layers, and (iii) models trained using different objective function exhibit different learning patterns and w.r.t the layer dropping.}
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


<h3 id="2022">2022</h3>

	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2022-12-EMNLP-transformation-latent-space-fine-tuned/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>On the Transformation of Latent Space in Fine-Tuned NLP Models</b></div>
		<div class="research-authors"> <i> Nadir Durrani, Hassan Sajjad, <span class='highlight-author'>Fahim Dalvi</span>, Firoj Alam </i> </div>
		
		<div class="research-venue"><a href='https://2022.emnlp.org'>The 2022 Conference on Empirical Methods in Natural Language Processing</a></div>
		<div class="research-abstract">	
			We study the evolution of latent space in fine-tuned NLP models. Different from the commonly used probing-framework, we opt for an unsupervised method to analyze representations. More specifically, we discover latent concepts in the representational space using hierarchical clustering. We then use an alignment function to gauge the similarity between the latent space of a pre-trained model and its fine-tuned version. We use traditional linguistic concepts to facilitate our understanding and also study how the model space transforms towards task-specific information. We perform a thorough analysis, comparing pre-trained and fine-tuned models across three models and three downstream tasks. The notable findings of our work are: i) the latent space of the higher layers evolve towards task-specific concepts, ii) whereas the lower layers retain generic concepts acquired in the pre-trained model, iii) we discovered that some concepts in the higher layers acquire polarity towards the output class, and iv) that these concepts can be used for generating adversarial triggers.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2022-12-EMNLP-transformation-latent-space-fine-tuned/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{durrani-etal-2022-transformation,
</span></span><span style="display:flex;"><span>    title = &#34;On the Transformation of Latent Space in Fine-Tuned {NLP} Models&#34;,
</span></span><span style="display:flex;"><span>    author = &#34;Durrani, Nadir  and
</span></span><span style="display:flex;"><span>      Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>      Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>      Alam, Firoj&#34;,
</span></span><span style="display:flex;"><span>    booktitle = &#34;Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing&#34;,
</span></span><span style="display:flex;"><span>    month = dec,
</span></span><span style="display:flex;"><span>    year = &#34;2022&#34;,
</span></span><span style="display:flex;"><span>    address = &#34;Abu Dhabi, United Arab Emirates&#34;,
</span></span><span style="display:flex;"><span>    publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    url = &#34;https://aclanthology.org/2022.emnlp-main.97&#34;,
</span></span><span style="display:flex;"><span>    doi = &#34;10.18653/v1/2022.emnlp-main.97&#34;,
</span></span><span style="display:flex;"><span>    pages = &#34;1495--1516&#34;,
</span></span><span style="display:flex;"><span>    abstract = &#34;We study the evolution of latent space in fine-tuned NLP models. Different from the commonly used probing-framework, we opt for an unsupervised method to analyze representations. More specifically, we discover latent concepts in the representational space using hierarchical clustering. We then use an alignment function to gauge the similarity between the latent space of a pre-trained model and its fine-tuned version. We use traditional linguistic concepts to facilitate our understanding and also study how the model space transforms towards task-specific information. We perform a thorough analysis, comparing pre-trained and fine-tuned models across three models and three downstream tasks. The notable findings of our work are: i) the latent space of the higher layers evolve towards task-specific concepts, ii) whereas the lower layers retain generic concepts acquired in the pre-trained model, iii) we discovered that some concepts in the higher layers acquire polarity towards the output class, and iv) that these concepts can be used for generating adversarial triggers.&#34;,
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2022-12-blackboxnlp-post-hoc-analysis-arabic-transformers/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Post-hoc analysis of Arabic transformer models</b></div>
		<div class="research-authors"> <i> Ahmed Abdelali, Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span>, Hassan Sajjad </i> </div>
		
		<div class="research-venue"><a href='https://blackboxnlp.github.io/2022/'>The Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</a></div>
		<div class="research-abstract">	
			Arabic is a Semitic language which is widely spoken with many dialects. Given the success of pre-trained language models, many transformer models trained on Arabic and its dialects have surfaced. While there have been an extrinsic evaluation of these models with respect to downstream NLP tasks, no work has been carried out to analyze and compare their internal representations. We probe how linguistic information is encoded in the transformer models, trained on different Arabic dialects. We perform a layer and neuron analysis on the models using morphological tagging tasks for different dialects of Arabic and a dialectal identification task. Our analysis enlightens interesting findings such as: i) word morphology is learned at the lower and middle layers, ii) while syntactic dependencies are predominantly captured at the higher layers, iii) despite a large overlap in their vocabulary, the MSA-based models fail to capture the nuances of Arabic dialects, iv) we found that neurons in embedding layers are polysemous in nature, while the neurons in middle layers are exclusive to specific properties.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2022-12-blackboxnlp-post-hoc-analysis-arabic-transformers/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{abdelali-etal-2022-post,
</span></span><span style="display:flex;"><span>    title = &#34;Post-hoc analysis of {A}rabic transformer models&#34;,
</span></span><span style="display:flex;"><span>    author = &#34;Abdelali, Ahmed  and
</span></span><span style="display:flex;"><span>      Durrani, Nadir  and
</span></span><span style="display:flex;"><span>      Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>      Sajjad, Hassan&#34;,
</span></span><span style="display:flex;"><span>    booktitle = &#34;Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP&#34;,
</span></span><span style="display:flex;"><span>    month = dec,
</span></span><span style="display:flex;"><span>    year = &#34;2022&#34;,
</span></span><span style="display:flex;"><span>    address = &#34;Abu Dhabi, United Arab Emirates (Hybrid)&#34;,
</span></span><span style="display:flex;"><span>    publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    url = &#34;https://aclanthology.org/2022.blackboxnlp-1.8&#34;,
</span></span><span style="display:flex;"><span>    doi = &#34;10.18653/v1/2022.blackboxnlp-1.8&#34;,
</span></span><span style="display:flex;"><span>    pages = &#34;91--103&#34;,
</span></span><span style="display:flex;"><span>    abstract = &#34;Arabic is a Semitic language which is widely spoken with many dialects. Given the success of pre-trained language models, many transformer models trained on Arabic and its dialects have surfaced. While there have been an extrinsic evaluation of these models with respect to downstream NLP tasks, no work has been carried out to analyze and compare their internal representations. We probe how linguistic information is encoded in the transformer models, trained on different Arabic dialects. We perform a layer and neuron analysis on the models using morphological tagging tasks for different dialects of Arabic and a dialectal identification task. Our analysis enlightens interesting findings such as: i) word morphology is learned at the lower and middle layers, ii) while syntactic dependencies are predominantly captured at the higher layers, iii) despite a large overlap in their vocabulary, the MSA-based models fail to capture the nuances of Arabic dialects, iv) we found that neurons in embedding layers are polysemous in nature, while the neurons in middle layers are exclusive to specific properties.&#34;,
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2022-12-WANLP-NatiQ-demo/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>NatiQ: An End-to-end Text-to-Speech System for Arabic</b></div>
		<div class="research-authors"> <i> Ahmed Abdelali, Nadir Durrani, Cenk Demiroglu, <span class='highlight-author'>Fahim Dalvi</span>, Hamdy Mubarak, Kareem Darwish </i> </div>
		
		<div class="research-venue"><a href='https://sites.google.com/view/wanlp2022/'>The Seventh Arabic Natural Language Processing Workshop (WANLP 2022)</a></div>
		<div class="research-abstract">	
			NatiQ is end-to-end text-to-speech system for Arabic. Our speech synthesizer uses an encoder-decoder architecture with attention. We used both tacotron-based models (tacotron- 1 and tacotron-2) and the faster transformer model for generating mel-spectrograms from characters. We concatenated Tacotron1 with the WaveRNN vocoder, Tacotron2 with the WaveGlow vocoder and ESPnet transformer with the parallel wavegan vocoder to synthesize waveforms from the spectrograms. We used in-house speech data for two voices: 1) neu- tral male {``}Hamza{&#39;&#39;}- narrating general content and news, and 2) expressive female {``}Amina{&#39;&#39;}- narrating children story books to train our models. Our best systems achieve an aver- age Mean Opinion Score (MOS) of 4.21 and 4.40 for Amina and Hamza respectively. The objective evaluation of the systems using word and character error rate (WER and CER) as well as the response time measured by real- time factor favored the end-to-end architecture ESPnet. NatiQ demo is available online at https://tts.qcri.org.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2022-12-WANLP-NatiQ-demo/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			<a class="research-action" href="https://tts.qcri.org">Demo</a> 
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{abdelali-etal-2022-natiq,
</span></span><span style="display:flex;"><span>    title = &#34;{N}ati{Q}: An End-to-end Text-to-Speech System for {A}rabic&#34;,
</span></span><span style="display:flex;"><span>    author = &#34;Abdelali, Ahmed  and
</span></span><span style="display:flex;"><span>      Durrani, Nadir  and
</span></span><span style="display:flex;"><span>      Demiroglu, Cenk  and
</span></span><span style="display:flex;"><span>      Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>      Mubarak, Hamdy  and
</span></span><span style="display:flex;"><span>      Darwish, Kareem&#34;,
</span></span><span style="display:flex;"><span>    booktitle = &#34;Proceedings of the The Seventh Arabic Natural Language Processing Workshop (WANLP)&#34;,
</span></span><span style="display:flex;"><span>    month = dec,
</span></span><span style="display:flex;"><span>    year = &#34;2022&#34;,
</span></span><span style="display:flex;"><span>    address = &#34;Abu Dhabi, United Arab Emirates (Hybrid)&#34;,
</span></span><span style="display:flex;"><span>    publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    url = &#34;https://aclanthology.org/2022.wanlp-1.38&#34;,
</span></span><span style="display:flex;"><span>    doi = &#34;10.18653/v1/2022.wanlp-1.38&#34;,
</span></span><span style="display:flex;"><span>    pages = &#34;394--398&#34;,
</span></span><span style="display:flex;"><span>    abstract = &#34;NatiQ is end-to-end text-to-speech system for Arabic. Our speech synthesizer uses an encoder-decoder architecture with attention. We used both tacotron-based models (tacotron- 1 and tacotron-2) and the faster transformer model for generating mel-spectrograms from characters. We concatenated Tacotron1 with the WaveRNN vocoder, Tacotron2 with the WaveGlow vocoder and ESPnet transformer with the parallel wavegan vocoder to synthesize waveforms from the spectrograms. We used in-house speech data for two voices: 1) neu- tral male {``}Hamza{&#39;&#39;}- narrating general content and news, and 2) expressive female {``}Amina{&#39;&#39;}- narrating children story books to train our models. Our best systems achieve an aver- age Mean Opinion Score (MOS) of 4.21 and 4.40 for Amina and Hamza respectively. The objective evaluation of the systems using word and character error rate (WER and CER) as well as the response time measured by real- time factor favored the end-to-end architecture ESPnet. NatiQ demo is available online at https://tts.qcri.org.&#34;,
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2022-11-TACL-neuron-level-interpretation-survey/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Neuron-level Interpretation of Deep NLP Models: A Survey</b></div>
		<div class="research-authors"> <i> Hassan Sajjad*, Nadir Durrani*, <span class='highlight-author'>Fahim Dalvi*</span> </i> </div>
		 <div class="research-author-note"> * These authors contributed equally to this work </div> 
		<div class="research-venue"><a href='https://direct.mit.edu/tacl/issue/volume/10'>Transactions of the Association for Computational Linguistics, Volume 10</a></div>
		<div class="research-abstract">	
			The proliferation of Deep Neural Networks in various domains has seen an increased need for interpretability of these models. Preliminary work done along this line, and papers that surveyed such, are focused on high-level representation analysis. However, a recent branch of work has concentrated on interpretability at a more granular level of analyzing neurons within these models. In this paper, we survey the work done on neuron analysis including: i) methods to discover and understand neurons in a network; ii) evaluation methods; iii) major findings including cross architectural comparisons that neuron analysis has unraveled; iv) applications of neuron probing such as: controlling the model, domain adaptation, and so forth; and v) a discussion on open issues and future research directions.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2022-11-TACL-neuron-level-interpretation-survey/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2022-11-TACL-neuron-level-interpretation-survey/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			<a class="research-action" href="https://youtu.be/I5pfZav8ix0">Video</a> 
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@article{sajjad-etal-2022-neuron,
</span></span><span style="display:flex;"><span>    title = &#34;Neuron-level Interpretation of Deep {NLP} Models: A Survey&#34;,
</span></span><span style="display:flex;"><span>    author = &#34;Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>      Durrani, Nadir  and
</span></span><span style="display:flex;"><span>      Dalvi, Fahim&#34;,
</span></span><span style="display:flex;"><span>    journal = &#34;Transactions of the Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    volume = &#34;10&#34;,
</span></span><span style="display:flex;"><span>    year = &#34;2022&#34;,
</span></span><span style="display:flex;"><span>    address = &#34;Cambridge, MA&#34;,
</span></span><span style="display:flex;"><span>    publisher = &#34;MIT Press&#34;,
</span></span><span style="display:flex;"><span>    url = &#34;https://aclanthology.org/2022.tacl-1.74&#34;,
</span></span><span style="display:flex;"><span>    doi = &#34;10.1162/tacl_a_00519&#34;,
</span></span><span style="display:flex;"><span>    pages = &#34;1285--1303&#34;,
</span></span><span style="display:flex;"><span>    abstract = &#34;The proliferation of Deep Neural Networks in various domains has seen an increased need for interpretability of these models. Preliminary work done along this line, and papers that surveyed such, are focused on high-level representation analysis. However, a recent branch of work has concentrated on interpretability at a more granular level of analyzing neurons within these models. In this paper, we survey the work done on neuron analysis including: i) methods to discover and understand neurons in a network; ii) evaluation methods; iii) major findings including cross architectural comparisons that neuron analysis has unraveled; iv) applications of neuron probing such as: controlling the model, domain adaptation, and so forth; and v) a discussion on open issues and future research directions.&#34;,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2022-10-COLING-effect-of-post-processing-contextualized/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Effect of Post-processing on Contextualized Word Representations</b></div>
		<div class="research-authors"> <i> Hassan Sajjad, Firoj Alam, <span class='highlight-author'>Fahim Dalvi</span>, Nadir Durrani </i> </div>
		
		<div class="research-venue"><a href='https://coling2022.org'>The 29th International Conference On Computational Linguistics</a></div>
		<div class="research-abstract">	
			Post-processing of static embedding has been shown to improve their performance on both lexical and sequence-level tasks. However, post-processing for contextualized embeddings is an under-studied problem. In this work, we question the usefulness of post-processing for contextualized embeddings obtained from different layers of pre-trained language models. More specifically, we standardize individual neuron activations using z-score, min-max normalization, and by removing top principal components using the all-but-the-top method. Additionally, we apply unit length normalization to word representations. On a diverse set of pre-trained models, we show that post-processing unwraps vital information present in the representations for both lexical tasks (such as word similarity and analogy) and sequence classification tasks. Our findings raise interesting points in relation to the research studies that use contextualized representations, and suggest z-score normalization as an essential step to consider when using them in an application.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2022-10-COLING-effect-of-post-processing-contextualized/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{sajjad-etal-2022-effect,
</span></span><span style="display:flex;"><span>    title = &#34;Effect of Post-processing on Contextualized Word Representations&#34;,
</span></span><span style="display:flex;"><span>    author = &#34;Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>      Alam, Firoj  and
</span></span><span style="display:flex;"><span>      Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>      Durrani, Nadir&#34;,
</span></span><span style="display:flex;"><span>    booktitle = &#34;Proceedings of the 29th International Conference on Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    month = oct,
</span></span><span style="display:flex;"><span>    year = &#34;2022&#34;,
</span></span><span style="display:flex;"><span>    address = &#34;Gyeongju, Republic of Korea&#34;,
</span></span><span style="display:flex;"><span>    publisher = &#34;International Committee on Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    url = &#34;https://aclanthology.org/2022.coling-1.277&#34;,
</span></span><span style="display:flex;"><span>    pages = &#34;3127--3142&#34;,
</span></span><span style="display:flex;"><span>    abstract = &#34;Post-processing of static embedding has been shown to improve their performance on both lexical and sequence-level tasks. However, post-processing for contextualized embeddings is an under-studied problem. In this work, we question the usefulness of post-processing for contextualized embeddings obtained from different layers of pre-trained language models. More specifically, we standardize individual neuron activations using z-score, min-max normalization, and by removing top principal components using the all-but-the-top method. Additionally, we apply unit length normalization to word representations. On a diverse set of pre-trained models, we show that post-processing unwraps vital information present in the representations for both lexical tasks (such as word similarity and analogy) and sequence classification tasks. Our findings raise interesting points in relation to the research studies that use contextualized representations, and suggest z-score normalization as an essential step to consider when using them in an application.&#34;,
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2022-07-NAACL-analyzing-encoded-concepts-transformers/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Analyzing Encoded Concepts in Transformer Language Models</b></div>
		<div class="research-authors"> <i> Hassan Sajjad, Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span>, Firoj Alam, Abdul Khan, Jia Xu </i> </div>
		
		<div class="research-venue"><a href='https://2022.naacl.org'>2022 Annual Conference of the North American Chapter of the Association for Computational Linguistics</a></div>
		<div class="research-abstract">	
			We propose a novel framework ConceptX, to analyze how latent concepts are encoded in representations learned within pre-trained lan-guage models. It uses clustering to discover the encoded concepts and explains them by aligning with a large set of human-defined concepts. Our analysis on seven transformer language models reveal interesting insights: i) the latent space within the learned representations overlap with different linguistic concepts to a varying degree, ii) the lower layers in the model are dominated by lexical concepts (e.g., affixation) and linguistic ontologies (e.g. Word-Net), whereas the core-linguistic concepts (e.g., morphology, syntactic relations) are better represented in the middle and higher layers, iii) some encoded concepts are multi-faceted and cannot be adequately explained using the existing human-defined concepts.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2022-07-NAACL-analyzing-encoded-concepts-transformers/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			<a class="research-action" href="https://github.com/hsajjad/ConceptX">Code</a> 
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{sajjad-etal-2022-analyzing,
</span></span><span style="display:flex;"><span>    title = &#34;Analyzing Encoded Concepts in Transformer Language Models&#34;,
</span></span><span style="display:flex;"><span>    author = &#34;Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>      Durrani, Nadir  and
</span></span><span style="display:flex;"><span>      Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>      Alam, Firoj  and
</span></span><span style="display:flex;"><span>      Khan, Abdul  and
</span></span><span style="display:flex;"><span>      Xu, Jia&#34;,
</span></span><span style="display:flex;"><span>    booktitle = &#34;Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies&#34;,
</span></span><span style="display:flex;"><span>    month = jul,
</span></span><span style="display:flex;"><span>    year = &#34;2022&#34;,
</span></span><span style="display:flex;"><span>    address = &#34;Seattle, United States&#34;,
</span></span><span style="display:flex;"><span>    publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    url = &#34;https://aclanthology.org/2022.naacl-main.225&#34;,
</span></span><span style="display:flex;"><span>    doi = &#34;10.18653/v1/2022.naacl-main.225&#34;,
</span></span><span style="display:flex;"><span>    pages = &#34;3082--3101&#34;,
</span></span><span style="display:flex;"><span>    abstract = &#34;We propose a novel framework ConceptX, to analyze how latent concepts are encoded in representations learned within pre-trained lan-guage models. It uses clustering to discover the encoded concepts and explains them by aligning with a large set of human-defined concepts. Our analysis on seven transformer language models reveal interesting insights: i) the latent space within the learned representations overlap with different linguistic concepts to a varying degree, ii) the lower layers in the model are dominated by lexical concepts (e.g., affixation) and linguistic ontologies (e.g. Word-Net), whereas the core-linguistic concepts (e.g., morphology, syntactic relations) are better represented in the middle and higher layers, iii) some encoded concepts are multi-faceted and cannot be adequately explained using the existing human-defined concepts.&#34;,
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2022-05-ICLR-discovering-latent-concepts-in-bert/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Discovering Latent Concepts Learned in BERT</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi*</span>, Abdul Rafae Khan*, Firoj Alam, Nadir Durrani, Jia Xu, Hassan Sajjad </i> </div>
		 <div class="research-author-note"> * These authors contributed equally to this work </div> 
		<div class="research-venue"><a href='https://iclr.cc/Conferences/2022'>The Tenth International Conference on Learning Representations </a></div>
		<div class="research-abstract">	
			A large number of studies that analyze deep neural network models and their ability to encode various linguistic and non-linguistic concepts provide an interpretation of the inner mechanics of these models. The scope of the analyses is limited to pre-defined concepts that reinforce the traditional linguistic knowledge and do not reflect on how novel concepts are learned by the model. We address this limitation by discovering and analyzing latent concepts learned in neural network models in an unsupervised fashion and provide interpretations from the model&#39;s perspective. In this work, we study: i) what latent concepts exist in the pre-trained BERT model, ii) how the discovered latent concepts align or diverge from classical linguistic hierarchy and iii) how the latent concepts evolve across layers. Our findings show: i) a model learns novel concepts (e.g. animal categories and demographic groups), which do not strictly adhere to any pre-defined categorization (e.g. POS, semantic tags), ii) several latent concepts are based on multiple properties which may include semantics, syntax, and  morphology, iii) the lower layers in the model dominate in learning shallow lexical concepts while the higher layers learn semantic relations and iv) the discovered  latent concepts highlight potential biases learned in the model. We also release a novel BERT ConceptNet dataset consisting of 174 concept labels and 1M annotated instances.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2022-05-ICLR-discovering-latent-concepts-in-bert/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2022-05-ICLR-discovering-latent-concepts-in-bert/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			<a class="research-action" href="https://neurox.qcri.org/projects/bert-concept-net.html">Resources</a> 
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{
</span></span><span style="display:flex;"><span>  dalvi2022discovering,
</span></span><span style="display:flex;"><span>  title={Discovering Latent Concepts Learned in {BERT}},
</span></span><span style="display:flex;"><span>  author={Fahim Dalvi
</span></span><span style="display:flex;"><span>    and Abdul Rafae Khan
</span></span><span style="display:flex;"><span>    and Firoj Alam
</span></span><span style="display:flex;"><span>    and Nadir Durrani
</span></span><span style="display:flex;"><span>    and Jia Xu
</span></span><span style="display:flex;"><span>    and Hassan Sajjad},
</span></span><span style="display:flex;"><span>  booktitle={International Conference on Learning Representations},
</span></span><span style="display:flex;"><span>  year={2022},
</span></span><span style="display:flex;"><span>  url={https://openreview.net/forum?id=POTMtpYI1xH}
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


<h3 id="2021">2021</h3>

	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2021-11-EMNLP-Findings-fighting-covid-infodemic/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society</b></div>
		<div class="research-authors"> <i> Firoj Alam, Shaden Shaar, <span class='highlight-author'>Fahim Dalvi</span>, Hassan Sajjad, Alex Nikolov, Hamdy Mubarak, Giovanni Da San Martino, Ahmed Abdelali, Nadir Durrani, Kareem Darwish, Abdulaziz Al-Homaid, Wajdi Zaghouani, Tommaso Caselli, Gijs Danoe, Friso Stolk, Britt Bruntink, Preslav Nakov </i> </div>
		
		<div class="research-venue"><a href='https://2021.emnlp.org'>Findings of the Association for Computational Linguistics: EMNLP 2021</a></div>
		<div class="research-abstract">	
			With the emergence of the COVID-19 pandemic, the political and the medical aspects of disinformation merged as the problem got elevated to a whole new level to become the first global infodemic. Fighting this infodemic has been declared one of the most important focus areas of the World Health Organization, with dangers ranging from promoting fake cures, rumors, and conspiracy theories to spreading xenophobia and panic. Addressing the issue requires solving a number of challenging problems such as identifying messages containing claims, determining their check-worthiness and factuality, and their potential to do harm as well as the nature of that harm, to mention just a few. To address this gap, we release a large dataset of 16K manually annotated tweets for fine-grained disinformation analysis that (i) focuses on COVID-19, (ii) combines the perspectives and the interests of journalists, fact-checkers, social media platforms, policy makers, and society, and (iii) covers Arabic, Bulgarian, Dutch, and English. Finally, we show strong evaluation results using pretrained Transformers, thus confirming the practical utility of the dataset in monolingual vs. multilingual, and single task vs. multitask settings.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2021-11-EMNLP-Findings-fighting-covid-infodemic/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			<a class="research-action" href="https://github.com/firojalam/COVID-19-disinformation">Code</a> 
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{alam-etal-2021-fighting-covid,
</span></span><span style="display:flex;"><span>    title = &#34;Fighting the {COVID}-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society&#34;,
</span></span><span style="display:flex;"><span>    author = &#34;Alam, Firoj  and
</span></span><span style="display:flex;"><span>      Shaar, Shaden  and
</span></span><span style="display:flex;"><span>      Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>      Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>      Nikolov, Alex  and
</span></span><span style="display:flex;"><span>      Mubarak, Hamdy  and
</span></span><span style="display:flex;"><span>      Da San Martino, Giovanni  and
</span></span><span style="display:flex;"><span>      Abdelali, Ahmed  and
</span></span><span style="display:flex;"><span>      Durrani, Nadir  and
</span></span><span style="display:flex;"><span>      Darwish, Kareem  and
</span></span><span style="display:flex;"><span>      Al-Homaid, Abdulaziz  and
</span></span><span style="display:flex;"><span>      Zaghouani, Wajdi  and
</span></span><span style="display:flex;"><span>      Caselli, Tommaso  and
</span></span><span style="display:flex;"><span>      Danoe, Gijs  and
</span></span><span style="display:flex;"><span>      Stolk, Friso  and
</span></span><span style="display:flex;"><span>      Bruntink, Britt  and
</span></span><span style="display:flex;"><span>      Nakov, Preslav&#34;,
</span></span><span style="display:flex;"><span>    booktitle = &#34;Findings of the Association for Computational Linguistics: EMNLP 2021&#34;,
</span></span><span style="display:flex;"><span>    month = nov,
</span></span><span style="display:flex;"><span>    year = &#34;2021&#34;,
</span></span><span style="display:flex;"><span>    address = &#34;Punta Cana, Dominican Republic&#34;,
</span></span><span style="display:flex;"><span>    publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    url = &#34;https://aclanthology.org/2021.findings-emnlp.56&#34;,
</span></span><span style="display:flex;"><span>    doi = &#34;10.18653/v1/2021.findings-emnlp.56&#34;,
</span></span><span style="display:flex;"><span>    pages = &#34;611--649&#34;,
</span></span><span style="display:flex;"><span>    abstract = &#34;With the emergence of the COVID-19 pandemic, the political and the medical aspects of disinformation merged as the problem got elevated to a whole new level to become the first global infodemic. Fighting this infodemic has been declared one of the most important focus areas of the World Health Organization, with dangers ranging from promoting fake cures, rumors, and conspiracy theories to spreading xenophobia and panic. Addressing the issue requires solving a number of challenging problems such as identifying messages containing claims, determining their check-worthiness and factuality, and their potential to do harm as well as the nature of that harm, to mention just a few. To address this gap, we release a large dataset of 16K manually annotated tweets for fine-grained disinformation analysis that (i) focuses on COVID-19, (ii) combines the perspectives and the interests of journalists, fact-checkers, social media platforms, policy makers, and society, and (iii) covers Arabic, Bulgarian, Dutch, and English. Finally, we show strong evaluation results using pretrained Transformers, thus confirming the practical utility of the dataset in monolingual vs. multilingual, and single task vs. multitask settings.&#34;,
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2021-08-ACL-Findings-transfer-learning-impact/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>How transfer learning impacts linguistic knowledge in deep NLP models?</b></div>
		<div class="research-authors"> <i> Nadir Durrani, Hassan Sajjad, <span class='highlight-author'>Fahim Dalvi</span> </i> </div>
		
		<div class="research-venue"><a href='https://2021.aclweb.org'>Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</a></div>
		<div class="research-abstract">	
			Transfer learning from pre-trained neural language models towards downstream tasks has been a predominant theme in NLP recently. Several researchers have shown that deep NLP models learn non-trivial amount of linguistic knowledge, captured at different layers of the model. We investigate how fine-tuning towards downstream NLP tasks impacts the learned linguistic knowledge. We carry out a study across popular pre-trained models BERT, RoBERTa and XLNet using layer and neuron-level diagnostic classifiers. We found that for some GLUE tasks, the network relies on the core linguistic information and preserve it deeper in the network, while for others it forgets. Linguistic information is distributed in the pre-trained language models but becomes localized to the lower layers post-fine-tuning, reserving higher layers for the task specific knowledge. The pattern varies across architectures, with BERT retaining linguistic information relatively deeper in the network compared to RoBERTa and XLNet, where it is predominantly delegated to the lower layers.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2021-08-ACL-Findings-transfer-learning-impact/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{durrani-etal-2021-transfer,
</span></span><span style="display:flex;"><span>    title = &#34;How transfer learning impacts linguistic knowledge in deep {NLP} models?&#34;,
</span></span><span style="display:flex;"><span>    author = &#34;Durrani, Nadir  and
</span></span><span style="display:flex;"><span>      Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>      Dalvi, Fahim&#34;,
</span></span><span style="display:flex;"><span>    booktitle = &#34;Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021&#34;,
</span></span><span style="display:flex;"><span>    month = aug,
</span></span><span style="display:flex;"><span>    year = &#34;2021&#34;,
</span></span><span style="display:flex;"><span>    address = &#34;Online&#34;,
</span></span><span style="display:flex;"><span>    publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    url = &#34;https://aclanthology.org/2021.findings-acl.438&#34;,
</span></span><span style="display:flex;"><span>    doi = &#34;10.18653/v1/2021.findings-acl.438&#34;,
</span></span><span style="display:flex;"><span>    pages = &#34;4947--4957&#34;,
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2021-06-ICWSM-fighting-covid-infodemic/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Fighting the COVID-19 Infodemic in Social Media: A Holistic Perspective and a Call to Arms</b></div>
		<div class="research-authors"> <i> Firoj Alam, <span class='highlight-author'>Fahim Dalvi</span>, Shaden Shaar, Nadir Durrani, Hamdy Mubarak, Alex Nikolov, Giovanni Da San Martino, Ahmed Abdelali, Hassan Sajjad, Kareem Darwish, Preslav Nakov </i> </div>
		
		<div class="research-venue"><a href='https://www.icwsm.org/2021/index.html'>15th International Conference On Web And Social Media</a></div>
		<div class="research-abstract">	
			With the outbreak of the COVID-19 pandemic, people turned to social media to read and to share timely information including statistics, warnings, advice, and inspirational stories. Unfortunately, alongside all this useful information, there was also a new blending of medical and political misinformation and disinformation, which gave rise to the first global infodemic. While fighting this infodemic is typically thought of in terms of factuality, the problem is much broader as malicious content includes not only fake news, rumors, and conspiracy theories, but also promotion of fake cures, panic, racism, xenophobia, and mistrust in the authorities, among others. This is a complex problem that needs a holistic approach combining the perspectives of journalists, fact-checkers, policymakers, government entities, social media platforms, and society as a whole. With this in mind, we define an annotation schema and detailed annotation instructions that reflect these perspectives. We further deploy a multilingual annotation platform, and we issue a call to arms to the research community and beyond to join the fight by supporting our crowdsourcing annotation efforts. We perform initial annotations using the annotation schema, and our initial experiments demonstrated sizable improvements over the baselines.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2021-06-ICWSM-fighting-covid-infodemic/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			<a class="research-action" href="http://doi.org/10.7910/DVN/XYK2UE">Data</a> 
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@article{Alam_covid_infodemic_2021,
</span></span><span style="display:flex;"><span>  title={Fighting the COVID-19 Infodemic in Social Media: A Holistic Perspective and a Call to Arms},
</span></span><span style="display:flex;"><span>  author={Alam, Firoj and Dalvi, Fahim and Shaar, Shaden and Durrani, Nadir and Mubarak, Hamdy and Nikolov, Alex and Da San Martino, Giovanni and Abdelali, Ahmed and Sajjad, Hassan and Darwish, Kareem and Nakov, Preslav},
</span></span><span style="display:flex;"><span>  volume={15},
</span></span><span style="display:flex;"><span>  url={https://ojs.aaai.org/index.php/ICWSM/article/view/18114},
</span></span><span style="display:flex;"><span>  number={1},
</span></span><span style="display:flex;"><span>  journal={Proceedings of the International AAAI Conference on Web and Social Media},
</span></span><span style="display:flex;"><span>  year={2021},
</span></span><span style="display:flex;"><span>  month={May},
</span></span><span style="display:flex;"><span>  pages={913-922},
</span></span><span style="display:flex;"><span>  abstractNote={With the outbreak of the COVID-19 pandemic, people turned to social media to read and to share timely information including statistics, warnings, advice, and inspirational stories. Unfortunately, alongside all this useful information, there was also a new blending of medical and political misinformation and disinformation, which gave rise to the first global infodemic. While fighting this infodemic is typically thought of in terms of factuality, the problem is much broader as malicious content includes not only fake news, rumors, and conspiracy theories, but also promotion of fake cures, panic, racism, xenophobia, and mistrust in the authorities, among others. This is a complex problem that needs a holistic approach combining the perspectives of journalists, fact-checkers, policymakers, government entities, social media platforms, and society as a whole. With this in mind, we define an annotation schema and detailed annotation instructions that reflect these perspectives. We further deploy a multilingual annotation platform, and we issue a call to arms to the research community and beyond to join the fight by supporting our crowdsourcing annotation efforts. We perform initial annotations using the annotation schema, and our initial experiments demonstrated sizable improvements over the baselines.}
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2021-06-NAACL-finegrained-interpretation-tutorial/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Fine-grained Interpretation and Causation Analysis in Deep NLP Models</b></div>
		<div class="research-authors"> <i> Hassan Sajjad, Narine Kokhlikyan, <span class='highlight-author'>Fahim Dalvi</span>, Nadir Durrani </i> </div>
		
		<div class="research-venue"><a href='https://2021.naacl.org'>2021 Annual Conference of the North American Chapter of the Association for Computational Linguistics</a></div>
		<div class="research-abstract">	
			Deep neural networks have constantly pushed the state-of-the-art performance in natural language processing and are considered as the de-facto modeling approach in solving complex NLP tasks such as machine translation, summarization and question-answering. Despite the proven efficacy of deep neural networks at-large, their opaqueness is a major cause of concern. In this tutorial, we will present research work on interpreting fine-grained components of a neural network model from two perspectives, i) fine-grained interpretation, and ii) causation analysis. The former is a class of methods to analyze neurons with respect to a desired language concept or a task. The latter studies the role of neurons and input features in explaining the decisions made by the model. We will also discuss how interpretation methods and causation analysis can connect towards better interpretability of model prediction. Finally, we will walk you through various toolkits that facilitate fine-grained interpretation and causation analysis of neural models.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2021-06-NAACL-finegrained-interpretation-tutorial/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			<a class="research-action" href="https://youtu.be/ayhBHZYjeqs">Video</a> 
			<a class="research-action" href="https://github.com/hsajjad/Interpretability-Tutorial-NAACL2021">Resources</a> 
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{sajjad-etal-2021-fine,
</span></span><span style="display:flex;"><span>  title = &#34;Fine-grained Interpretation and Causation Analysis in Deep {NLP} Models&#34;,
</span></span><span style="display:flex;"><span>  author = &#34;Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>    Kokhlikyan, Narine  and
</span></span><span style="display:flex;"><span>    Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>    Durrani, Nadir&#34;,
</span></span><span style="display:flex;"><span>  booktitle = &#34;Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorials&#34;,
</span></span><span style="display:flex;"><span>  month = jun,
</span></span><span style="display:flex;"><span>  year = &#34;2021&#34;,
</span></span><span style="display:flex;"><span>  address = &#34;Online&#34;,
</span></span><span style="display:flex;"><span>  publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>  url = &#34;https://www.aclweb.org/anthology/2021.naacl-tutorials.2&#34;,
</span></span><span style="display:flex;"><span>  pages = &#34;5--10&#34;,
</span></span><span style="display:flex;"><span>  abstract = &#34;Deep neural networks have constantly pushed the state-of-the-art performance in natural language processing and are considered as the de-facto modeling approach in solving complex NLP tasks such as machine translation, summarization and question-answering. Despite the proven efficacy of deep neural networks at-large, their opaqueness is a major cause of concern. In this tutorial, we will present research work on interpreting fine-grained components of a neural network model from two perspectives, i) fine-grained interpretation, and ii) causation analysis. The former is a class of methods to analyze neurons with respect to a desired language concept or a task. The latter studies the role of neurons and input features in explaining the decisions made by the model. We will also discuss how interpretation methods and causation analysis can connect towards better interpretability of model prediction. Finally, we will walk you through various toolkits that facilitate fine-grained interpretation and causation analysis of neural models.&#34;,
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


<h3 id="2020">2020</h3>

	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2020-12-COLING-arabench/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>AraBench: Benchmarking Dialectal Arabic-English Machine Translation</b></div>
		<div class="research-authors"> <i> Hassan Sajjad, Ahmed Abdelali, Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span> </i> </div>
		
		<div class="research-venue"><a href='https://coling2020.org'>The 28th International Conference on Computational Linguistics</a></div>
		<div class="research-abstract">	
			Low-resource machine translation suffers from the scarcity of training data and the unavailability of standard evaluation sets. While a number of research efforts target the former, the unavailability of evaluation benchmarks remain a major hindrance in tracking the progress in low-resource machine translation. In this paper, we introduce AraBench, an evaluation suite for dialectal Arabic to English machine translation. Compared to Modern Standard Arabic, Arabic dialects are challenging due to their spoken nature, non-standard orthography, and a large variation in dialectness. To this end, we pool together already available Dialectal Arabic-English resources and additionally build novel test sets. AraBench offers 4 coarse, 15 fine-grained and 25 city-level dialect categories, belonging to diverse genres, such as media, chat, religion and travel with varying level of dialectness. We report strong baselines using several training settings: fine-tuning, back-translation and data augmentation. The evaluation suite opens a wide range of research frontiers to push efforts in low-resource machine translation, particularly Arabic dialect translation. The evaluation suite and the dialectal system are publicly available for research purposes.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2020-12-COLING-arabench/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			<a class="research-action" href="https://alt.qcri.org/resources1/mt/arabench/">Data</a> 
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{sajjad-etal-2020-arabench,
</span></span><span style="display:flex;"><span>    title = &#34;{A}ra{B}ench: Benchmarking Dialectal {A}rabic-{E}nglish Machine Translation&#34;,
</span></span><span style="display:flex;"><span>    author = &#34;Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>      Abdelali, Ahmed  and
</span></span><span style="display:flex;"><span>      Durrani, Nadir  and
</span></span><span style="display:flex;"><span>      Dalvi, Fahim&#34;,
</span></span><span style="display:flex;"><span>    booktitle = &#34;Proceedings of the 28th International Conference on Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    month = dec,
</span></span><span style="display:flex;"><span>    year = &#34;2020&#34;,
</span></span><span style="display:flex;"><span>    address = &#34;Barcelona, Spain (Online)&#34;,
</span></span><span style="display:flex;"><span>    publisher = &#34;International Committee on Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    url = &#34;https://www.aclweb.org/anthology/2020.coling-main.447&#34;,
</span></span><span style="display:flex;"><span>    doi = &#34;10.18653/v1/2020.coling-main.447&#34;,
</span></span><span style="display:flex;"><span>    pages = &#34;5094--5107&#34;,
</span></span><span style="display:flex;"><span>    abstract = &#34;Low-resource machine translation suffers from the scarcity of training data and the unavailability of standard evaluation sets. While a number of research efforts target the former, the unavailability of evaluation benchmarks remain a major hindrance in tracking the progress in low-resource machine translation. In this paper, we introduce AraBench, an evaluation suite for dialectal Arabic to English machine translation. Compared to Modern Standard Arabic, Arabic dialects are challenging due to their spoken nature, non-standard orthography, and a large variation in dialectness. To this end, we pool together already available Dialectal Arabic-English resources and additionally build novel test sets. AraBench offers 4 coarse, 15 fine-grained and 25 city-level dialect categories, belonging to diverse genres, such as media, chat, religion and travel with varying level of dialectness. We report strong baselines using several training settings: fine-tuning, back-translation and data augmentation. The evaluation suite opens a wide range of research frontiers to push efforts in low-resource machine translation, particularly Arabic dialect translation. The evaluation suite and the dialectal system are publicly available for research purposes.&#34;,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2020-11-EMNLP-analyzing-indiividual-neurons-transformers/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Analyzing Individual Neurons in Pre-trained Language Models</b></div>
		<div class="research-authors"> <i> Nadir Durrani, Hassan Sajjad, <span class='highlight-author'>Fahim Dalvi</span>, Yonatan Belinkov </i> </div>
		
		<div class="research-venue"><a href='https://2020.emnlp.org'>The 2020 Conference on Empirical Methods in Natural Language Processing</a></div>
		<div class="research-abstract">	
			While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons. We carry out a neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pretrained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2020-11-EMNLP-analyzing-indiividual-neurons-transformers/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{durrani-etal-2020-analyzing,
</span></span><span style="display:flex;"><span>    title = &#34;Analyzing Individual Neurons in Pre-trained Language Models&#34;,
</span></span><span style="display:flex;"><span>    author = &#34;Durrani, Nadir  and
</span></span><span style="display:flex;"><span>      Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>      Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>      Belinkov, Yonatan&#34;,
</span></span><span style="display:flex;"><span>    booktitle = &#34;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)&#34;,
</span></span><span style="display:flex;"><span>    month = nov,
</span></span><span style="display:flex;"><span>    year = &#34;2020&#34;,
</span></span><span style="display:flex;"><span>    address = &#34;Online&#34;,
</span></span><span style="display:flex;"><span>    publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    url = &#34;https://www.aclweb.org/anthology/2020.emnlp-main.395&#34;,
</span></span><span style="display:flex;"><span>    doi = &#34;10.18653/v1/2020.emnlp-main.395&#34;,
</span></span><span style="display:flex;"><span>    pages = &#34;4865--4880&#34;,
</span></span><span style="display:flex;"><span>    abstract = &#34;While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.&#34;,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2020-11-EMNLP-analyzing-redundancy-pretrained-models/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Analyzing Redundancy in Pretrained Transformer Models</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Hassan Sajjad, Nadir Durrani, Yonatan Belinkov </i> </div>
		
		<div class="research-venue"><a href='https://2020.emnlp.org'>The 2020 Conference on Empirical Methods in Natural Language Processing</a></div>
		<div class="research-abstract">	
			Transformer-based deep NLP models are trained using hundreds of millions of parameters, limiting their applicability in computationally constrained environments. In this paper, we study the cause of these limitations by defining a notion of Redundancy, which we categorize into two classes: General Redundancy and Task-specific Redundancy. We dissect two popular pretrained models, BERT and XLNet, studying how much redundancy they exhibit at a representation-level and at a more fine-grained neuron-level. Our analysis reveals interesting insights, such as: i) 85% of the neurons across the network are redundant and ii) at least 92% of them can be removed when optimizing towards a downstream task. Based on our analysis, we present an efficient feature-based transfer learning procedure, which maintains 97% performance while using at-most 10% of the original neurons.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2020-11-EMNLP-analyzing-redundancy-pretrained-models/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			<a class="research-action" href="https://github.com/fdalvi/analyzing-redundancy-in-pretrained-transformer-models">Code</a> 
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{dalvi-etal-2020-analyzing,
</span></span><span style="display:flex;"><span>    title = &#34;Analyzing Redundancy in Pretrained Transformer Models&#34;,
</span></span><span style="display:flex;"><span>    author = &#34;Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>      Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>      Durrani, Nadir  and
</span></span><span style="display:flex;"><span>      Belinkov, Yonatan&#34;,
</span></span><span style="display:flex;"><span>    booktitle = &#34;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)&#34;,
</span></span><span style="display:flex;"><span>    month = nov,
</span></span><span style="display:flex;"><span>    year = &#34;2020&#34;,
</span></span><span style="display:flex;"><span>    address = &#34;Online&#34;,
</span></span><span style="display:flex;"><span>    publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    url = &#34;https://www.aclweb.org/anthology/2020.emnlp-main.398&#34;,
</span></span><span style="display:flex;"><span>    doi = &#34;10.18653/v1/2020.emnlp-main.398&#34;,
</span></span><span style="display:flex;"><span>    pages = &#34;4908--4926&#34;,
</span></span><span style="display:flex;"><span>    abstract = &#34;Transformer-based deep NLP models are trained using hundreds of millions of parameters, limiting their applicability in computationally constrained environments. In this paper, we study the cause of these limitations by defining a notion of Redundancy, which we categorize into two classes: General Redundancy and Task-specific Redundancy. We dissect two popular pretrained models, BERT and XLNet, studying how much redundancy they exhibit at a representation-level and at a more fine-grained neuron-level. Our analysis reveals interesting insights, such as i) 85{<span style="color:#66d9ef">\%</span>} of the neurons across the network are redundant and ii) at least 92{<span style="color:#66d9ef">\%</span>} of them can be removed when optimizing towards a downstream task. Based on our analysis, we present an efficient feature-based transfer learning procedure, which maintains 97{<span style="color:#66d9ef">\%</span>} performance while using at-most 10{<span style="color:#66d9ef">\%</span>} of the original neurons.&#34;,
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2020-07-ACL-similarity-analysis/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Similarity Analysis of Contextual Word Representation Models</b></div>
		<div class="research-authors"> <i> John Wu*, Yonatan Belinkov*, Hassan Sajjad, Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span>, James Glass </i> </div>
		 <div class="research-author-note"> * These authors contributed equally to this work </div> 
		<div class="research-venue"><a href='http://acl2020.org'>58th Annual Meeting of the Association for Computational Linguistics</a></div>
		<div class="research-abstract">	
			This paper investigates contextual word representation models from the lens of similarity analysis. Given a collection of trained models, we measure the similarity of their internal representations and attention. Critically, these models come from vastly different architectures. We use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic annotation. The analysis reveals that models within the same family are more similar to one another, as may be expected. Surprisingly, different architectures have rather similar representations, but different individual neurons. We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2020-07-ACL-similarity-analysis/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{wu-etal-2020-similarity,
</span></span><span style="display:flex;"><span>    title = &#34;Similarity Analysis of Contextual Word Representation Models&#34;,
</span></span><span style="display:flex;"><span>    author = &#34;Wu, John  and
</span></span><span style="display:flex;"><span>      Belinkov, Yonatan  and
</span></span><span style="display:flex;"><span>      Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>      Durrani, Nadir  and
</span></span><span style="display:flex;"><span>      Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>      Glass, James&#34;,
</span></span><span style="display:flex;"><span>    booktitle = &#34;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    month = jul,
</span></span><span style="display:flex;"><span>    year = &#34;2020&#34;,
</span></span><span style="display:flex;"><span>    address = &#34;Online&#34;,
</span></span><span style="display:flex;"><span>    publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    url = &#34;https://www.aclweb.org/anthology/2020.acl-main.422&#34;,
</span></span><span style="display:flex;"><span>    doi = &#34;10.18653/v1/2020.acl-main.422&#34;,
</span></span><span style="display:flex;"><span>    pages = &#34;4638--4655&#34;,
</span></span><span style="display:flex;"><span>    abstract = &#34;This paper investigates contextual word representation models from the lens of similarity analysis. Given a collection of trained models, we measure the similarity of their internal representations and attention. Critically, these models come from vastly different architectures. We use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic annotation. The analysis reveals that models within the same family are more similar to one another, as may be expected. Surprisingly, different architectures have rather similar representations, but different individual neurons. We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks.&#34;,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2020-03-CL-linguistic-representations-nmt/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>On the Linguistic Representational Power of Neural Machine Translation Models</b></div>
		<div class="research-authors"> <i> Yonatan Belinkov*, Nadir Durrani*, <span class='highlight-author'>Fahim Dalvi</span>, Hassan Sajjad, James Glass </i> </div>
		 <div class="research-author-note"> * These authors contributed equally to this work </div> 
		<div class="research-venue"><a href='https://www.aclweb.org/anthology/events/cl-2020/'> Computational Linguistics, Volume 46, Issue 1 </a></div>
		<div class="research-abstract">	
			Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models trained in an end-to-end fashion, without being provided any direct supervision during the training process, learn a non-trivial amount of linguistic information. Notable findings include the following observations: (i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers of the model; (iii) Representations learned using characters are more informed about word-morphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2020-03-CL-linguistic-representations-nmt/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@article{belinkov-etal-2020-linguistic,
</span></span><span style="display:flex;"><span>    title = &#34;On the Linguistic Representational Power of Neural Machine Translation Models&#34;,
</span></span><span style="display:flex;"><span>    author = &#34;Belinkov, Yonatan  and
</span></span><span style="display:flex;"><span>      Durrani, Nadir  and
</span></span><span style="display:flex;"><span>      Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>      Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>      Glass, James&#34;,
</span></span><span style="display:flex;"><span>    journal = &#34;Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>    volume = &#34;46&#34;,
</span></span><span style="display:flex;"><span>    number = &#34;1&#34;,
</span></span><span style="display:flex;"><span>    month = mar,
</span></span><span style="display:flex;"><span>    year = &#34;2020&#34;,
</span></span><span style="display:flex;"><span>    url = &#34;https://www.aclweb.org/anthology/2020.cl-1.1&#34;,
</span></span><span style="display:flex;"><span>    doi = &#34;10.1162/coli_a_00367&#34;,
</span></span><span style="display:flex;"><span>    pages = &#34;1--52&#34;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


<h3 id="2019">2019</h3>

	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2019-09-OSNEM-rumour-verification/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Rumour verification through recurring information and an inner-attention mechanism</b></div>
		<div class="research-authors"> <i> Ahmet Aker, Alfred Sliwa, <span class='highlight-author'>Fahim Dalvi</span>, Kalina Bontcheva </i> </div>
		
		<div class="research-venue"><a href='https://www.sciencedirect.com/journal/online-social-networks-and-media/vol/13/suppl/C'> Online Social Networks and Media Volume 13 </a></div>
		<div class="research-abstract">	
			Verification of online rumours is becoming an increasingly important task with the prevalence of event discussions on social media platforms. This paper proposes an inner-attention-based neural network model that uses frequent, recurring terms from past rumours to classify a newly emerging rumour as true, false or unverified. Unlike other methods proposed in related work, our model uses the source rumour alone without any additional information, such as user replies to the rumour or additional feature engineering. Our method outperforms the current state-of-the-art methods on benchmark datasets (RumourEval2017) by 3% accuracy and 6% F-1 leading to 60.7% accuracy and 61.6% F-1. We also compare our attention-based method to two similar models which however do not make use of recurrent terms. The attention-based method guided by frequent recurring terms outperforms this baseline on the same dataset, indicating that the recurring terms injected by the attention mechanism have high positive impact on distinguishing between true and false rumours. Furthermore, we perform out-of-domain evaluations and show that our model is indeed highly competitive compared to the baselines on a newly released RumourEval2019 dataset and also achieves the best performance on classifying fake and legitimate news headlines.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2019-09-OSNEM-rumour-verification/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@article{AKER2019100045,
</span></span><span style="display:flex;"><span>  title = &#34;Rumour verification through recurring information and an inner-attention mechanism&#34;,
</span></span><span style="display:flex;"><span>  journal = &#34;Online Social Networks and Media&#34;,
</span></span><span style="display:flex;"><span>  volume = &#34;13&#34;,
</span></span><span style="display:flex;"><span>  pages = &#34;100045&#34;,
</span></span><span style="display:flex;"><span>  year = &#34;2019&#34;,
</span></span><span style="display:flex;"><span>  issn = &#34;2468-6964&#34;,
</span></span><span style="display:flex;"><span>  doi = &#34;https://doi.org/10.1016/j.osnem.2019.07.001&#34;,
</span></span><span style="display:flex;"><span>  url = &#34;http://www.sciencedirect.com/science/article/pii/S2468696419300588&#34;,
</span></span><span style="display:flex;"><span>  author = &#34;Ahmet Aker and Alfred Sliwa and Fahim Dalvi and Kalina Bontcheva&#34;,
</span></span><span style="display:flex;"><span>  keywords = &#34;Rumour Verification, Inner Attention Model, Recurring Terms in Rumours&#34;
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2019-06-NAACL-comparing-nmt-granularity/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>One Size Does Not Fit All: Comparing NMT Representations of Different Granularities</b></div>
		<div class="research-authors"> <i> Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span>, Hassan Sajjad, Yonatan Belinkov, Preslav Nakov </i> </div>
		
		<div class="research-venue"><a href='https://www.aclweb.org/anthology/events/naacl-2019/'> The 17th Annual Conference of the North American Chapter of the Association for Computational Linguistics </a></div>
		<div class="research-abstract">	
			Recent work has shown that contextualized word representations derived from neural machine translation are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model morphology, syntax, and semantics. We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2019-06-NAACL-comparing-nmt-granularity/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{durrani-etal-2019-one,
</span></span><span style="display:flex;"><span>  title = &#34;One Size Does Not Fit All: Comparing {NMT} Representations of Different Granularities&#34;,
</span></span><span style="display:flex;"><span>  author = &#34;Durrani, Nadir  and
</span></span><span style="display:flex;"><span>    Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>    Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>    Belinkov, Yonatan  and
</span></span><span style="display:flex;"><span>    Nakov, Preslav&#34;,
</span></span><span style="display:flex;"><span>  booktitle = &#34;Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&#34;,
</span></span><span style="display:flex;"><span>  month = jun,
</span></span><span style="display:flex;"><span>  year = &#34;2019&#34;,
</span></span><span style="display:flex;"><span>  address = &#34;Minneapolis, Minnesota&#34;,
</span></span><span style="display:flex;"><span>  publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>  url = &#34;https://www.aclweb.org/anthology/N19-1154&#34;,
</span></span><span style="display:flex;"><span>  doi = &#34;10.18653/v1/N19-1154&#34;,
</span></span><span style="display:flex;"><span>  pages = &#34;1504--1516&#34;,
</span></span><span style="display:flex;"><span>  abstract = &#34;Recent work has shown that contextualized word representations derived from neural machine translation are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model morphology, syntax, and semantics. We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.&#34;,
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2019-05-ICLR-unsupervised-neurons/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Identifying And Controlling Important Neurons In Neural Machine Translation</b></div>
		<div class="research-authors"> <i> Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span>, James Glass </i> </div>
		
		<div class="research-venue"><a href='https://iclr.cc/Conferences/2019/CallForPapers'> Seventh International Conference on Learning Representations </a></div>
		<div class="research-abstract">	
			Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons. We develop unsupervised methods for discovering important neurons in NMT models. Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision. We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena. Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2019-05-ICLR-unsupervised-neurons/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			<a class="research-action" href="https://github.com/fdalvi/NeuroX">Code</a> 
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{
</span></span><span style="display:flex;"><span>  bau2018identifying,
</span></span><span style="display:flex;"><span>  title={Identifying and Controlling Important Neurons in Neural Machine Translation},
</span></span><span style="display:flex;"><span>  author={Anthony Bau
</span></span><span style="display:flex;"><span>    and Yonatan Belinkov
</span></span><span style="display:flex;"><span>    and Hassan Sajjad
</span></span><span style="display:flex;"><span>    and Nadir Durrani
</span></span><span style="display:flex;"><span>    and Fahim Dalvi
</span></span><span style="display:flex;"><span>    and James Glass},
</span></span><span style="display:flex;"><span>  booktitle={International Conference on Learning Representations},
</span></span><span style="display:flex;"><span>  year={2019},
</span></span><span style="display:flex;"><span>  url={https://openreview.net/forum?id=H1z-PsR5KX},
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2019-02-AAAI-individual-neurons/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi*</span>, Nadir Durrani*, Hassan Sajjad*, Yonatan Belinkov, Anthony Bau, James Glass </i> </div>
		 <div class="research-author-note"> * These authors contributed equally to this work </div> 
		<div class="research-venue"><a href='https://aaai.org/Conferences/AAAI-19/'> The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19) </a></div>
		<div class="research-abstract">	
			Despite the remarkable evolution of deep neural networks in natural language processing (NLP), their interpretability remains a challenge. Previous work largely focused on what these models learn at the representation level. We break this analysis down further and study individual dimensions (neurons) in the vector representation learned by end-to-end neural models in NLP tasks. We propose two methods: Linguistic Correlation Analysis, based on a supervised method to extract the most relevant neurons with respect to an extrinsic task, and Cross-model Correlation Analysis, an unsupervised method to extract salient neurons w.r.t. the model itself. We evaluate the effectiveness of our techniques by ablating the identified neurons and reevaluating the network’s performance for two tasks: neural machine translation (NMT) and neural language modeling (NLM). We further present a comprehensive analysis of neurons with the aim to address the following questions: i) how localized or distributed are different linguistic properties in the models? ii) are certain neurons exclusive to some properties and not others? iii) is the information more or less distributed in NMT vs. NLM? and iv) how important are the neurons identified through the linguistic correlation method to the overall task? Our code is publicly available as part of the NeuroX toolkit (Dalvi et al. 2019a). This paper is a non-archived version of the paper published at AAAI (Dalvi et al. 2019b).
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2019-02-AAAI-individual-neurons/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2019-02-AAAI-individual-neurons/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			<a class="research-action" href="https://github.com/fdalvi/NeuroX">Code</a> 
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@article{dalvi2019individualneurons, 
</span></span><span style="display:flex;"><span>  title={What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models},
</span></span><span style="display:flex;"><span>  author={Dalvi, Fahim and Durrani, Nadir and Sajjad, Hassan and Belinkov, Yonatan and Bau, Anthony and Glass, James},
</span></span><span style="display:flex;"><span>  volume={33},
</span></span><span style="display:flex;"><span>  url={https://ojs.aaai.org/index.php/AAAI/article/view/4592},
</span></span><span style="display:flex;"><span>  DOI={10.1609/aaai.v33i01.33016309},
</span></span><span style="display:flex;"><span>  number={01},
</span></span><span style="display:flex;"><span>  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
</span></span><span style="display:flex;"><span>  year={2019},
</span></span><span style="display:flex;"><span>  month={Jul.},
</span></span><span style="display:flex;"><span>  pages={6309-6317},
</span></span><span style="display:flex;"><span>  abstractNote={Despite the remarkable evolution of deep neural networks in natural language processing (NLP), their interpretability remains a challenge. Previous work largely focused on what these models learn at the representation level. We break this analysis down further and study individual dimensions (neurons) in the vector representation learned by end-to-end neural models in NLP tasks. We propose two methods: &amp;lt;em&amp;gt;Linguistic Correlation Analysis&amp;lt;/em&amp;gt;, based on a supervised method to extract the most relevant neurons with respect to an extrinsic task, and &amp;lt;em&amp;gt;Cross-model Correlation Analysis&amp;lt;/em&amp;gt;, an unsupervised method to extract salient neurons w.r.t. the model itself. We evaluate the effectiveness of our techniques by ablating the identified neurons and reevaluating the network’s performance for two tasks: neural machine translation (NMT) and neural language modeling (NLM). We further present a comprehensive analysis of neurons with the aim to address the following questions: i) how localized or distributed are different linguistic properties in the models? ii) are certain neurons exclusive to some properties and not others? iii) is the information more or less distributed in NMT vs. NLM? and iv) how important are the neurons identified through the linguistic correlation method to the overall task? Our code is publicly available as part of the NeuroX toolkit (Dalvi et al. 2019a). This paper is a non-archived version of the paper published at AAAI (Dalvi et al. 2019b).}
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2019-02-AAAI-neurox/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>NeuroX: A Toolkit for Analyzing Individual Neurons in Neural Networks</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Avery Nortonsmith, Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, James Glass </i> </div>
		
		<div class="research-venue"><a href='https://aaai.org/Conferences/AAAI-19/'> The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19) </a></div>
		<div class="research-abstract">	
			We present a toolkit to facilitate the interpretation and understanding of neural network models. The toolkit provides several methods to identify salient neurons with respect to the model itself or an external task. A user can visualize selected neurons, ablate them to measure their effect on the model accuracy, and manipulate them to control the behavior of the model at the test time. Such an analysis has a potential to serve as a springboard in various research directions, such as understanding the model, better architectural choices, model distillation and controlling data biases. The toolkit is available for download.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2019-02-AAAI-neurox/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2019-02-AAAI-individual-neurons/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			<a class="research-action" href="https://github.com/fdalvi/NeuroX">Code</a> 
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@article{dalvi2019neurox,
</span></span><span style="display:flex;"><span>  title={NeuroX: A Toolkit for Analyzing Individual Neurons in Neural Networks},
</span></span><span style="display:flex;"><span>  author={Dalvi, Fahim and Nortonsmith, Avery and Bau, Anthony and Belinkov, Yonatan and Sajjad, Hassan and Durrani, Nadir and Glass, James},
</span></span><span style="display:flex;"><span>  volume={33},
</span></span><span style="display:flex;"><span>  url={https://ojs.aaai.org/index.php/AAAI/article/view/5063},
</span></span><span style="display:flex;"><span>  DOI={10.1609/aaai.v33i01.33019851},
</span></span><span style="display:flex;"><span>  number={01},
</span></span><span style="display:flex;"><span>  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
</span></span><span style="display:flex;"><span>  year={2019},
</span></span><span style="display:flex;"><span>  month={Jul.},
</span></span><span style="display:flex;"><span>  pages={9851-9852},
</span></span><span style="display:flex;"><span>  abstractNote={We present a toolkit to facilitate the interpretation and understanding of neural network models. The toolkit provides several methods to identify salient neurons with respect to the model itself or an external task. A user can visualize selected neurons, ablate them to measure their effect on the model accuracy, and manipulate them to control the behavior of the model at the test time. Such an analysis has a potential to serve as a springboard in various research directions, such as understanding the model, better architectural choices, model distillation and controlling data biases. The toolkit is available for download.}
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


<h3 id="2018">2018</h3>

	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2018-10-LCN-group-identification-crowd/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Group Identification in Crowded Environments Using Proximity Sensing</b></div>
		<div class="research-authors"> <i> Shaden Shaar, Saquib Razak, <span class='highlight-author'>Fahim Dalvi</span>, Syed Ali Hashim Moosavi </i> </div>
		
		<div class="research-venue"><a href='https://ieeexplore.ieee.org/servlet/opac?punumber=8635823'> 43rd IEEE Conference on Local Computer Networks </a></div>
		<div class="research-abstract">	
			Children and elderly separating from their family members is a common phenomenon, especially in crowded environments. In order to avoid this problem, places like Disney World and pilgrimage officials have developed systems like wearable tags to determine groups or families. These tags require information about families to be entered manually, either by the users or the facility organizers. The information, if correct, can then be used to help identify and locate a lost person&#39;s group. Manually entering information is inefficient, and usually leads to either long waiting times during entry, or partial information entry within the tags. In this paper, we propose a system that uses proximity sensing to determine groups and families without any input or interaction with the user. In our system, each user is given a wearable device that keeps track of it&#39;s neighbors using bluetooth transmissions. The system then uses this proximity data to predict cliques that represent family members.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2018-10-LCN-group-identification-crowd/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@InProceedings{shaar2018group,
</span></span><span style="display:flex;"><span>  title={Group Identification in Crowded Environments Using Proximity Sensing},
</span></span><span style="display:flex;"><span>  author={Shaar, Shaden and Razak, Saquib and Dalvi, Fahim and Moosavi, Syed Ali Hashim},
</span></span><span style="display:flex;"><span>  booktitle={43rd {IEEE} Conference on Local Computer Networks, {LCN} 2018, Chicago, IL, USA, October 1-4, 2018},
</span></span><span style="display:flex;"><span>  pages={319--322},
</span></span><span style="display:flex;"><span>  year={2018},
</span></span><span style="display:flex;"><span>  organization={IEEE},
</span></span><span style="display:flex;"><span>  url={https://doi.org/10.1109/LCN.2018.8638142},
</span></span><span style="display:flex;"><span>  doi={10.1109/LCN.2018.8638142}
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2018-06-NAACL-stream-decoding/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Incremental Decoding and Training Methods for Simultaneous Translation in Neural Machine Translation</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi*</span>, Nadir Durrani*, Hassan Sajjad, Stephan Vogel </i> </div>
		 <div class="research-author-note"> * These authors contributed equally to this work </div> 
		<div class="research-venue"><a href='http://naacl2018.org'> The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics </a></div>
		<div class="research-abstract">	
			We address the problem of simultaneous translation by modifying the Neural MT decoder to operate with dynamically built encoder and attention. We propose a tunable agent which decides the best segmentation strategy for a userdefined BLEU loss and Average Proportion (AP) constraint. Our agent outperforms previously proposed Wait-if-diff and Wait-if-worse agents (Cho and Esipova, 2016) on BLEU with a lower latency. Secondly we proposed datadriven changes to Neural MT training to better match the incremental decoding framework.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2018-06-NAACL-stream-decoding/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			<a class="research-action" href="https://github.com/fdalvi/seq2seq-attn-stream">Code</a> 
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{dalvi-etal-2018-incremental,
</span></span><span style="display:flex;"><span>  title = &#34;Incremental Decoding and Training Methods for Simultaneous Translation in Neural Machine Translation&#34;,
</span></span><span style="display:flex;"><span>  author = &#34;Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>    Durrani, Nadir  and
</span></span><span style="display:flex;"><span>    Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>    Vogel, Stephan&#34;,
</span></span><span style="display:flex;"><span>  booktitle = &#34;Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)&#34;,
</span></span><span style="display:flex;"><span>  month = jun,
</span></span><span style="display:flex;"><span>  year = &#34;2018&#34;,
</span></span><span style="display:flex;"><span>  address = &#34;New Orleans, Louisiana&#34;,
</span></span><span style="display:flex;"><span>  publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>  url = &#34;https://www.aclweb.org/anthology/N18-2079&#34;,
</span></span><span style="display:flex;"><span>  doi = &#34;10.18653/v1/N18-2079&#34;,
</span></span><span style="display:flex;"><span>  pages = &#34;493--499&#34;,
</span></span><span style="display:flex;"><span>  abstract = &#34;We address the problem of simultaneous translation by modifying the Neural MT decoder to operate with dynamically built encoder and attention. We propose a tunable agent which decides the best segmentation strategy for a user-defined BLEU loss and Average Proportion (AP) constraint. Our agent outperforms previously proposed Wait-if-diff and Wait-if-worse agents (Cho and Esipova, 2016) on BLEU with a lower latency. Secondly we proposed data-driven changes to Neural MT training to better match the incremental decoding framework.&#34;,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2018-03-NEWSIR-qclusty/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Qlusty: Quick and Dirty Generation of Event Videos from Written Media Coverage</b></div>
		<div class="research-authors"> <i> Alberto Barrón-Cedeño, Giovanni Da San Martino, Yifan Zhang, Ahmed Ali, <span class='highlight-author'>Fahim Dalvi</span> </i> </div>
		
		<div class="research-venue"><a href='http://www.ecir2018.org/programme/ecir-workshop/#NewsIR&#x27;18'> NewsIR @ 40th European Conference on Information Retrieval </a></div>
		<div class="research-abstract">	
			Qlusty generates videos describing the coverage of the same event by different news outlets automatically. Throughout four modules it identifies events, de-duplicates notes, ranks according to coverage, and queries for images to generate an overview video. In this manuscript we present our preliminary models, including quantitative evaluations of the former two and a qualitative analysis of the latter two. The results show the potential for achieving our main aim: contributing in breaking the information bubble, so common in the current news landscape.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2018-03-NEWSIR-qclusty/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@article{barron2018qlusty,
</span></span><span style="display:flex;"><span>  title={Qlusty: Quick and Dirty Generation of Event Videos from Written Media Coverage.},
</span></span><span style="display:flex;"><span>  author={Barr{<span style="color:#66d9ef">\&#39;</span>o}n-Cede{<span style="color:#66d9ef">\~</span>n}o, Alberto 
</span></span><span style="display:flex;"><span>    and Da San Martino, Giovanni
</span></span><span style="display:flex;"><span>    and Zhang, Yifan
</span></span><span style="display:flex;"><span>    and Ali, Ahmed M
</span></span><span style="display:flex;"><span>    and Dalvi, Fahim},
</span></span><span style="display:flex;"><span>  journal={NewsIR@ ECIR},
</span></span><span style="display:flex;"><span>  volume={2079},
</span></span><span style="display:flex;"><span>  pages={27--32},
</span></span><span style="display:flex;"><span>  year={2018}
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


<h3 id="2017">2017</h3>

	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2017-12-IWSLT-multidomain-nmt/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Neural Machine Translation Training in a Multi-Domain Scenario</b></div>
		<div class="research-authors"> <i> Hassan Sajjad, Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span>, Yonatan Belinkov, Stephan Vogel </i> </div>
		
		<div class="research-venue"><a href='http://workshop2017.iwslt.org'>International Workshop on Spoken Language Translation 2017</a></div>
		<div class="research-abstract">	
			In this paper, we explore alternative ways to train a neural machine translation system in a multi-domain scenario. We investigate data concatenation (with fine tuning), model stacking (multi-level fine tuning), data selection and weighted ensemble. Our findings show that the best translation quality can be achieved by building an initial system on a concatenation of available out-of-domain data and then fine-tuning it on in-domain data. Model stacking works best when training begins with the furthest out-of-domain data and the model is incrementally fine-tuned with the next furthest domain and so on. Data selection did not give the best results, but can be considered as a decent compromise between training time and translation quality. A weighted ensemble of different individual models performed better than data selection. It is beneficial in a scenario when there is no time for fine-tuning.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2017-12-IWSLT-multidomain-nmt/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2017-12-IWSLT-multidomain-nmt/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{sajjad2017iwslt,
</span></span><span style="display:flex;"><span>  title={Neural Machine Translation Training in a Multi-Domain Scenario},
</span></span><span style="display:flex;"><span>  author={Sajjad, Hassan and Durrani, Nadir and Dalvi, Fahim and Belinkov, Yonatan and Vogel, Stephan},
</span></span><span style="display:flex;"><span>  booktitle={International Workshop on Spoken Language Translation},
</span></span><span style="display:flex;"><span>  year={2017}
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2017-12-IWSLT-reordering-models/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Continuous Space Reordering Models for Phrase-based MT</b></div>
		<div class="research-authors"> <i> Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span> </i> </div>
		
		<div class="research-venue"><a href='http://workshop2017.iwslt.org'>International Workshop on Spoken Language Translation 2017</a></div>
		<div class="research-abstract">	
			Bilingual sequence models improve phrase-based translation and reordering by overcoming phrasal independence assumption and handling long range reordering. However, due to data sparsity, these models often fall back to very small context sizes. This problem has been previously addressed by learning sequences over generalized representations such as POS tags or word clusters. In this paper, we explore an alternative based on neural network models. More concretely we train neuralized versions of lexicalized reordering and the operation sequence models using feed-forward neural network. Our results show improvements of up to 0.6 and 0.5 BLEU points on top of the baseline German→English and English→German systems. We also observed improvements compared to the systems that used POS tags and word clusters to train these models. Because we modify the bilingual corpus to integrate reordering operations, this allows us to also train a sequence-to-sequence neural MT model having explicit reordering triggers. Our motivation was to directly enable reordering information in the encoder-decoder framework, which otherwise relies solely on the attention model to handle long range reordering. We tried both coarser and fine-grained reordering operations. However, these experiments did not yield any improvements over the baseline Neural MT systems.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2017-12-IWSLT-reordering-models/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{durrani2017iwslt,
</span></span><span style="display:flex;"><span>  title={Continuous Space Reordering Models for Phrase-based MT},
</span></span><span style="display:flex;"><span>  author={Durrani, Nadir and Dalvi, Fahim},
</span></span><span style="display:flex;"><span>  booktitle={International Workshop on Spoken Language Translation},
</span></span><span style="display:flex;"><span>  year={2017}
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2017-11-IJCNLP-improving-nmt/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Understanding and Improving Morphological Learning in the Neural Machine Translation Decoder</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, Stephan Vogel </i> </div>
		
		<div class="research-venue"><a href='http://ijcnlp2017.org'> 8th International Joint Conference on Natural Language Processing </a></div>
		<div class="research-abstract">	
			End-to-end training makes the neural machine translation (NMT) architecture simpler, yet elegant compared to traditional statistical machine translation (SMT). However, little is known about linguistic patterns of morphology, syntax and semantics learned during the training of NMT systems, and more importantly, which parts of the architecture are responsible for learning each of these phenomena. In this paper we i) analyze how much morphology an NMT decoder learns, and ii) investigate whether injecting target morphology into the decoder helps it produce better translations. To this end we present three methods: i) joint generation, ii) joint-data learning, and iii) multi-task learning. Our results show that explicit morphological information helps the decoder learn target language morphology and improves the translation quality by 0.2–0.6 BLEU points.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2017-11-IJCNLP-improving-nmt/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			<a class="research-action" href="/research/2017-11-IJCNLP-improving-nmt/slides.pdf">Slides</a> 
			<a class="research-action" href="https://github.com/fdalvi/seq2seq-attn-multitask">Code</a> 
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{dalvi-etal-2017-understanding,
</span></span><span style="display:flex;"><span>  title = &#34;Understanding and Improving Morphological Learning in the Neural Machine Translation Decoder&#34;,
</span></span><span style="display:flex;"><span>  author = &#34;Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>    Durrani, Nadir  and
</span></span><span style="display:flex;"><span>    Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>    Belinkov, Yonatan  and
</span></span><span style="display:flex;"><span>    Vogel, Stephan&#34;,
</span></span><span style="display:flex;"><span>  booktitle = &#34;Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)&#34;,
</span></span><span style="display:flex;"><span>  month = nov,
</span></span><span style="display:flex;"><span>  year = &#34;2017&#34;,
</span></span><span style="display:flex;"><span>  address = &#34;Taipei, Taiwan&#34;,
</span></span><span style="display:flex;"><span>  publisher = &#34;Asian Federation of Natural Language Processing&#34;,
</span></span><span style="display:flex;"><span>  url = &#34;https://www.aclweb.org/anthology/I17-1015&#34;,
</span></span><span style="display:flex;"><span>  pages = &#34;142--151&#34;,
</span></span><span style="display:flex;"><span>  abstract = &#34;End-to-end training makes the neural machine translation (NMT) architecture simpler, yet elegant compared to traditional statistical machine translation (SMT). However, little is known about linguistic patterns of morphology, syntax and semantics learned during the training of NMT systems, and more importantly, which parts of the architecture are responsible for learning each of these phenomenon. In this paper we i) analyze how much morphology an NMT decoder learns, and ii) investigate whether injecting target morphology in the decoder helps it to produce better translations. To this end we present three methods: i) simultaneous translation, ii) joint-data learning, and iii) multi-task learning. Our results show that explicit morphological information helps the decoder learn target language morphology and improves the translation quality by 0.2{--}0.6 BLEU points.&#34;,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2017-11-IJCNLP-nmt-layers/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks</b></div>
		<div class="research-authors"> <i> Yonatan Belinkov, Lluís Màrquez, Hassan Sajjad, Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span>, James Glass </i> </div>
		
		<div class="research-venue"><a href='http://ijcnlp2017.org'> 8th International Joint Conference on Natural Language Processing </a></div>
		<div class="research-abstract">	
			While neural machine translation (NMT) models provide improved translation quality in an elegant framework, it is less clear what they learn about language. Recent work has started evaluating the quality of vector representations learned by NMT models on morphological and syntactic tasks. In this paper, we investigate the representations learned at different layers of NMT encoders. We train NMT systems on parallel data and use the models to extract features for training a classifier on two tasks: part-of-speech and semantic tagging. We then measure the performance of the classifier as a proxy to the quality of the original NMT model for the given task. Our quantitative analysis yields interesting insights regarding representation learning in NMT models. For instance, we find that higher layers are better at learning semantics while lower layers tend to be better for part-of-speech tagging. We also observe little effect of the target language on source-side representations, especially in higher quality models.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2017-11-IJCNLP-nmt-layers/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{belinkov-etal-2017-evaluating,
</span></span><span style="display:flex;"><span>  title = &#34;Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks&#34;,
</span></span><span style="display:flex;"><span>  author = &#34;Belinkov, Yonatan  and
</span></span><span style="display:flex;"><span>    M{<span style="color:#66d9ef">\`</span>a}rquez, Llu{<span style="color:#66d9ef">\&#39;\i</span>}s  and
</span></span><span style="display:flex;"><span>    Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>    Durrani, Nadir  and
</span></span><span style="display:flex;"><span>    Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>    Glass, James&#34;,
</span></span><span style="display:flex;"><span>  booktitle = &#34;Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)&#34;,
</span></span><span style="display:flex;"><span>  month = nov,
</span></span><span style="display:flex;"><span>  year = &#34;2017&#34;,
</span></span><span style="display:flex;"><span>  address = &#34;Taipei, Taiwan&#34;,
</span></span><span style="display:flex;"><span>  publisher = &#34;Asian Federation of Natural Language Processing&#34;,
</span></span><span style="display:flex;"><span>  url = &#34;https://www.aclweb.org/anthology/I17-1001&#34;,
</span></span><span style="display:flex;"><span>  pages = &#34;1--10&#34;,
</span></span><span style="display:flex;"><span>  abstract = &#34;While neural machine translation (NMT) models provide improved translation quality in an elegant framework, it is less clear what they learn about language. Recent work has started evaluating the quality of vector representations learned by NMT models on morphological and syntactic tasks. In this paper, we investigate the representations learned at different layers of NMT encoders. We train NMT systems on parallel data and use the models to extract features for training a classifier on two tasks: part-of-speech and semantic tagging. We then measure the performance of the classifier as a proxy to the quality of the original NMT model for the given task. Our quantitative analysis yields interesting insights regarding representation learning in NMT models. For instance, we find that higher layers are better at learning semantics while lower layers tend to be better for part-of-speech tagging. We also observe little effect of the target language on source-side representations, especially in higher quality models.&#34;,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2017-07-ACL-challenging-morphology/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging</b></div>
		<div class="research-authors"> <i> Hassan Sajjad, <span class='highlight-author'>Fahim Dalvi</span>, Nadir Durrani, Ahmed Abdelali, Yonatan Belinkov, Stephan Vogel  </i> </div>
		
		<div class="research-venue"><a href='http://acl2017.org'>55th Annual Meeting of the Association for Computational Linguistics</a></div>
		<div class="research-abstract">	
			Word segmentation plays a pivotal role in improving any Arabic NLP application. Therefore, a lot of research has been spent in improving its accuracy. Off-the-shelf tools, however, are: i) complicated to use and ii) domain/dialect dependent. We explore three language-independent alternatives to morphological segmentation using: i) data-driven sub-word units, ii) characters as a unit of learning, and iii) word embeddings learned using a character CNN (Convolution Neural Network). On the tasks of Machine Translation and POS tagging, we found these methods to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2017-07-ACL-challenging-morphology/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2017-07-ACL-challenging-morphology/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{sajjad-etal-2017-challenging,
</span></span><span style="display:flex;"><span>  title = &#34;Challenging Language-Dependent Segmentation for {A}rabic: An Application to Machine Translation and Part-of-Speech Tagging&#34;,
</span></span><span style="display:flex;"><span>  author = &#34;Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>    Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>    Durrani, Nadir  and
</span></span><span style="display:flex;"><span>    Abdelali, Ahmed  and
</span></span><span style="display:flex;"><span>    Belinkov, Yonatan  and
</span></span><span style="display:flex;"><span>    Vogel, Stephan&#34;,
</span></span><span style="display:flex;"><span>  booktitle = &#34;Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)&#34;,
</span></span><span style="display:flex;"><span>  month = jul,
</span></span><span style="display:flex;"><span>  year = &#34;2017&#34;,
</span></span><span style="display:flex;"><span>  address = &#34;Vancouver, Canada&#34;,
</span></span><span style="display:flex;"><span>  publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>  url = &#34;https://www.aclweb.org/anthology/P17-2095&#34;,
</span></span><span style="display:flex;"><span>  doi = &#34;10.18653/v1/P17-2095&#34;,
</span></span><span style="display:flex;"><span>  pages = &#34;601--607&#34;,
</span></span><span style="display:flex;"><span>  abstract = &#34;Word segmentation plays a pivotal role in improving any Arabic NLP application. Therefore, a lot of research has been spent in improving its accuracy. Off-the-shelf tools, however, are: i) complicated to use and ii) domain/dialect dependent. We explore three language-independent alternatives to morphological segmentation using: i) data-driven sub-word units, ii) characters as a unit of learning, and iii) word embeddings learned using a character CNN (Convolution Neural Network). On the tasks of Machine Translation and POS tagging, we found these methods to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance.&#34;,
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2017-07-ACL-nmt-morphology/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>What do Neural Machine Translation Models Learn about Morphology?</b></div>
		<div class="research-authors"> <i> Yonatan Belinkov, Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span>, Hassan Sajjad, James Glass   </i> </div>
		
		<div class="research-venue"><a href='http://acl2017.org'>55th Annual Meeting of the Association for Computational Linguistics</a></div>
		<div class="research-abstract">	
			Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2017-07-ACL-nmt-morphology/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2017-07-ACL-nmt-morphology/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{belinkov-etal-2017-neural,
</span></span><span style="display:flex;"><span>  title = &#34;What do Neural Machine Translation Models Learn about Morphology?&#34;,
</span></span><span style="display:flex;"><span>  author = &#34;Belinkov, Yonatan  and
</span></span><span style="display:flex;"><span>    Durrani, Nadir  and
</span></span><span style="display:flex;"><span>    Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>    Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>    Glass, James&#34;,
</span></span><span style="display:flex;"><span>  booktitle = &#34;Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&#34;,
</span></span><span style="display:flex;"><span>  month = jul,
</span></span><span style="display:flex;"><span>  year = &#34;2017&#34;,
</span></span><span style="display:flex;"><span>  address = &#34;Vancouver, Canada&#34;,
</span></span><span style="display:flex;"><span>  publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>  url = &#34;https://www.aclweb.org/anthology/P17-1080&#34;,
</span></span><span style="display:flex;"><span>  doi = &#34;10.18653/v1/P17-1080&#34;,
</span></span><span style="display:flex;"><span>  pages = &#34;861--872&#34;,
</span></span><span style="display:flex;"><span>  abstract = &#34;Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure.&#34;,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2017-04-EACL-Demo/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>QCRI&#39;s Live Speech Translation System</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Yifan Zhang, Sameer Khurana, Nadir Durrani, Hassan Sajjad Ahmed Abdelali, Hamdy Mubarak, Ahmed Ali, Stephan Vogel </i> </div>
		
		<div class="research-venue"><a href='http://eacl2017.org'>15th Conference of the European Chapter of the Association for Computational Linguistics</a></div>
		<div class="research-abstract">	
			We present QCRI’s Arabic-to-English speech translation system. It features modern web technologies to capture live audio, and broadcasts Arabic transcriptions and English translations simultaneously. Our Kaldi-based ASR system uses the Time Delay Neural Network architecture, while our Machine Translation (MT) system uses both phrase-based and neural frameworks. Although our neural MT system is slower than the phrase-based system, it produces significantly better translations and is memory efficient.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2017-04-EACL-Demo/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2017-04-EACL-Demo/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{dalvi-etal-2017-qcri,
</span></span><span style="display:flex;"><span>  title = &#34;{QCRI} Live Speech Translation System&#34;,
</span></span><span style="display:flex;"><span>  author = &#34;Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>    Zhang, Yifan  and
</span></span><span style="display:flex;"><span>    Khurana, Sameer  and
</span></span><span style="display:flex;"><span>    Durrani, Nadir  and
</span></span><span style="display:flex;"><span>    Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>    Abdelali, Ahmed  and
</span></span><span style="display:flex;"><span>    Mubarak, Hamdy  and
</span></span><span style="display:flex;"><span>    Ali, Ahmed  and
</span></span><span style="display:flex;"><span>    Vogel, Stephan&#34;,
</span></span><span style="display:flex;"><span>  booktitle = &#34;Proceedings of the Software Demonstrations of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>  month = apr,
</span></span><span style="display:flex;"><span>  year = &#34;2017&#34;,
</span></span><span style="display:flex;"><span>  address = &#34;Valencia, Spain&#34;,
</span></span><span style="display:flex;"><span>  publisher = &#34;Association for Computational Linguistics&#34;,
</span></span><span style="display:flex;"><span>  url = &#34;https://www.aclweb.org/anthology/E17-3016&#34;,
</span></span><span style="display:flex;"><span>  pages = &#34;61--64&#34;,
</span></span><span style="display:flex;"><span>  abstract = &#34;This paper presents QCRI{&#39;}s Arabic-to-English live speech translation system. It features modern web technologies to capture live audio, and broadcasts Arabic transcriptions and English translations simultaneously. Our Kaldi-based ASR system uses the Time Delay Neural Network (TDNN) architecture, while our Machine Translation (MT) system uses both phrase-based and neural frameworks. Although our neural MT system is slower than the phrase-based system, it produces significantly better translations and is memory efficient. The demo is available at <span style="color:#66d9ef">\url</span>{https://st.qcri.org/demos/livetranslation}.&#34;,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


<h3 id="2016">2016</h3>

	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2016-12-DSL-Dialect-ID/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>QCRI @ DSL 2016: Spoken Arabic Dialect Identification Using Textual Features</b></div>
		<div class="research-authors"> <i> Mohamed Eldesouki, <span class='highlight-author'>Fahim Dalvi</span>, Hassan Sajjad, and Kareem Darwish </i> </div>
		
		<div class="research-venue"><a href='http://ttg.uni-saarland.de/vardial2016/'>The Third Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial3)</a></div>
		<div class="research-abstract">	
			The paper describes the QCRI submissions to the shared task of automatic Arabic dialect classification into 5 Arabic variants, namely Egyptian, Gulf, Levantine, North-African (Maghrebi), and Modern Standard Arabic (MSA). The relatively small training set is automatically generated from an ASR system. To avoid over-fitting on such small data, we selected and designed features that capture the morphological essence of the different dialects. We submitted four runs to the Arabic sub-task. For all runs, we used a combined feature vector of character bigrams, trigrams, 4-grams, and 5-grams. We tried several machine-learning algorithms, namely Logistic Regres- sion, Naive Bayes, Neural Networks, and Support Vector Machines (SVM) with linear and string kernels. Our submitted runs used SVM with a linear kernel. In the closed submission, we got the best accuracy of 0.5136 and the third best weighted F1 score, with a difference of less than 0.002 from the best system.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2016-12-DSL-Dialect-ID/paper.pdf">PDF</a> 
			
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{eldesouki-etal-2016-qcri,
</span></span><span style="display:flex;"><span>  title = &#34;{QCRI} @ {DSL} 2016: Spoken {A}rabic Dialect Identification Using Textual Features&#34;,
</span></span><span style="display:flex;"><span>  author = &#34;Eldesouki, Mohamed  and
</span></span><span style="display:flex;"><span>    Dalvi, Fahim  and
</span></span><span style="display:flex;"><span>    Sajjad, Hassan  and
</span></span><span style="display:flex;"><span>    Darwish, Kareem&#34;,
</span></span><span style="display:flex;"><span>  booktitle = &#34;Proceedings of the Third Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial3)&#34;,
</span></span><span style="display:flex;"><span>  month = dec,
</span></span><span style="display:flex;"><span>  year = &#34;2016&#34;,
</span></span><span style="display:flex;"><span>  address = &#34;Osaka, Japan&#34;,
</span></span><span style="display:flex;"><span>  publisher = &#34;The COLING 2016 Organizing Committee&#34;,
</span></span><span style="display:flex;"><span>  url = &#34;https://www.aclweb.org/anthology/W16-4828&#34;,
</span></span><span style="display:flex;"><span>  pages = &#34;221--226&#34;,
</span></span><span style="display:flex;"><span>  abstract = &#34;The paper describes the QCRI submissions to the task of automatic Arabic dialect classification into 5 Arabic variants, namely Egyptian, Gulf, Levantine, North-African, and Modern Standard Arabic (MSA). The training data is relatively small and is automatically generated from an ASR system. To avoid over-fitting on such small data, we carefully selected and designed the features to capture the morphological essence of the different dialects. We submitted four runs to the Arabic sub-task. For all runs, we used a combined feature vector of character bi-grams, tri-grams, 4-grams, and 5-grams. We tried several machine-learning algorithms, namely Logistic Regression, Naive Bayes, Neural Networks, and Support Vector Machines (SVM) with linear and string kernels. However, our submitted runs used SVM with a linear kernel. In the closed submission, we got the best accuracy of 0.5136 and the third best weighted F1 score, with a difference less than 0.002 from the highest score.&#34;,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/2016-12-IWSLT/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>QCRI Machine Translation Systems for IWSLT 16</b></div>
		<div class="research-authors"> <i> Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span>, Hassan Sajjad, Stephan Vogel </i> </div>
		
		<div class="research-venue"><a href='http://workshop2016.iwslt.org'>International Workshop on Spoken Language Translation 2016</a></div>
		<div class="research-abstract">	
			This paper describes QCRI’s machine translation systems for the IWSLT 2016 evaluation campaign. We participated in the Arabic→English and English→Arabic tracks. We built both Phrase-based and Neural machine translation models, in an effort to probe whether the newly emerged NMT framework surpasses the traditional phrase-based systems in Arabic-English language pairs. We trained a very strong phrase-based system including, a big language model, the Operation Sequence Model, Neural Network Joint Model and Class-based models along with different domain adaptation techniques such as MML filtering, mixture modeling and using fine tuning over NNJM model. However, a Neural MT system, trained by stacking data from different genres through fine-tuning, and applying ensemble over 8 models, beat our very strong phrase-based system by a significant 2 BLEU points margin in Arabic→English direction. We did not obtain similar gains in the other direction but were still able to outperform the phrase-based system. We also applied system combination on phrase-based and NMT outputs.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/2016-12-IWSLT/paper.pdf">PDF</a> 
			<a class="research-action" href="/research/2016-12-IWSLT/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			<a class="research-action" href="/research/2016-12-IWSLT/slides.pdf">Slides</a> 
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@inproceedings{durrani2016iwslt,
</span></span><span style="display:flex;"><span>  title={QCRI Machine Translation Systems for IWSLT 16},
</span></span><span style="display:flex;"><span>  author={Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Vogel, Stephan},
</span></span><span style="display:flex;"><span>  booktitle={International Workshop on Spoken Language Translation},
</span></span><span style="display:flex;"><span>  year={2016}
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
			</div>
		
	</div>
</div>


<h3 id="unpublished-works">Unpublished works</h3>
<p><em>A list of unpublished work that resulted from student research or class projects</em></p>

	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2016-stanford-ee267/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>VirtualWars: Towards a More Immersive VR Experience</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Tariq Patanam </i> </div>
		
		<div class="research-venue"><a href='https://stanford.edu/class/ee267/'>EE267: Virtual Reality, Stanford, 2016</a></div>
		<div class="research-abstract">	
			Ensuring that virtual reality experiences are immersive is key to ensuring the success of VR and even VR. However, despite impressive commercial advancements from the Oculus Rift to the HTC Vive, a number of inherent limitations remain when comparing virtual experiences to real experiences: field of view, limb (mainly hand) tracking, position tracking in the world, haptic feedback, and more. In this study we seek to test a number of creative workarounds to create a fully immersive experience with current technological limitations. We found that overall, immersive experiences could be created, but because of the limitations of the technology, limitations had to be imposed on the virtual world such as how the content had to be presented (interactively and not passively), how objects were destroyed, and more.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/unpublished-2016-stanford-ee267/report.pdf">PDF</a> 
			<a class="research-action" href="/research/unpublished-2016-stanford-ee267/poster.pdf">Poster</a> 
			
			
			
			
			
			
			
		</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2016-stanford-231n/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>DeepFace: Face Generation using Deep Learning</b></div>
		<div class="research-authors"> <i> Hardie Cate, <span class='highlight-author'>Fahim Dalvi</span>, Zeshan Hussain </i> </div>
		
		<div class="research-venue"><a href='http://cs231n.stanford.edu'>CS231n: Convolutional Neural Networks for Visual Recognition, Stanford, 2016</a></div>
		<div class="research-abstract">	
			Convolutional neural networks (CNNs) are powerful tools for image classification and object detection, but they can also be used to generate images. For our project, we use CNNs to create a face generation system. Given a set of desired facial characteristics, we produce a well-formed face that matches these attributes. Potential facial char- acteristics fall within the general categories of raw at- tributes (e.g., big nose, brown hair, etc.), ethnicity (e.g., white, black, Indian), and accessories (e.g. sunglasses, hat, etc.). In our face generation system, we fine-tune a convolutional network pre-trained on faces to create a binary classification system for the potential facial charac- teristics. We then employ a novel technique that models feature activations as a custom Gaussian Mixture Model in order to identify relevant features for feature inversion. Our face generation system has many potential uses, in- cluding identifying suspects in law enforcement settings.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/unpublished-2016-stanford-231n/report.pdf">PDF</a> 
			<a class="research-action" href="/research/unpublished-2016-stanford-231n/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@article{DBLP:journals/corr/CateDH17a,
</span></span><span style="display:flex;"><span>  author    = {Hardie Cate and
</span></span><span style="display:flex;"><span>               Fahim Dalvi and
</span></span><span style="display:flex;"><span>               Zeshan Hussain},
</span></span><span style="display:flex;"><span>  title     = {DeepFace: Face Generation using Deep Learning},
</span></span><span style="display:flex;"><span>  journal   = {CoRR},
</span></span><span style="display:flex;"><span>  volume    = {abs/1701.01876},
</span></span><span style="display:flex;"><span>  year      = {2017},
</span></span><span style="display:flex;"><span>  url       = {http://arxiv.org/abs/1701.01876},
</span></span><span style="display:flex;"><span>  archivePrefix = {arXiv},
</span></span><span style="display:flex;"><span>  eprint    = {1701.01876},
</span></span><span style="display:flex;"><span>  timestamp = {Wed, 07 Jun 2017 14:40:49 +0200},
</span></span><span style="display:flex;"><span>  biburl    = {https://dblp.org/rec/bib/journals/corr/CateDH17a},
</span></span><span style="display:flex;"><span>  bibsource = {dblp computer science bibliography, https://dblp.org}
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2015-stanford-229/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Sign Language Recognition using Temporal Classification</b></div>
		<div class="research-authors"> <i> Hardie Cate, <span class='highlight-author'>Fahim Dalvi</span>, Zeshan Hussain </i> </div>
		
		<div class="research-venue"><a href='http://cs229.stanford.edu'>CS229: Machine Learning, Stanford, 2015</a></div>
		<div class="research-abstract">	
			In the US alone, there are approximately 900,000 hearing impaired people whose primary mode of conversation is sign language. For these people, communication with non-signers is a daily struggle, and they are often disadvantaged when it comes to finding a job, accessing health care, etc. There are a few emerging technologies aimed at overcoming these communication barriers, but most existing solutions rely on cameras to translate sign language into vocal language. While these solutions are promising, they require the hearing impaired person to carry the technology with him/her or for a proper environment to be set up for translation. One alternative is to move the technology onto the person’s body. Devices like the Myo armband available in the market today enable us to collect data about the position of the user’s hands and fingers over time. Since each sign is roughly a combination of gestures across time, we can use these technologies for sign language translation. For our project, we utilize a dataset collected by a group at the University of South Wales, which contains parameters, such as hand position, hand rotation, and finger bend, for 95 unique signs. For each input stream representing a sign, we predict which sign class this stream falls into. We begin by implementing baseline SVM and logistic regression models, which perform reasonably well on high-quality data. Lower quality data requires a more sophisticated approach, so we explore different methods in temporal classification, including long short-term memory architectures and sequential pattern mining methods.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/unpublished-2015-stanford-229/report.pdf">PDF</a> 
			<a class="research-action" href="/research/unpublished-2015-stanford-229/poster.pdf">Poster</a> 
			<div class="research-action" onclick="this.parentNode.nextElementSibling.classList.toggle('expanded')">Cite (.bib)</div> 
			
			
			
			
			
			
		</div>
		
			<div class="research-citation">	
				<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>@article{DBLP:journals/corr/CateDH17,
</span></span><span style="display:flex;"><span>  author    = {Hardie Cate and
</span></span><span style="display:flex;"><span>               Fahim Dalvi and
</span></span><span style="display:flex;"><span>               Zeshan Hussain},
</span></span><span style="display:flex;"><span>  title     = {Sign Language Recognition Using Temporal Classification},
</span></span><span style="display:flex;"><span>  journal   = {CoRR},
</span></span><span style="display:flex;"><span>  volume    = {abs/1701.01875},
</span></span><span style="display:flex;"><span>  year      = {2017},
</span></span><span style="display:flex;"><span>  url       = {http://arxiv.org/abs/1701.01875},
</span></span><span style="display:flex;"><span>  archivePrefix = {arXiv},
</span></span><span style="display:flex;"><span>  eprint    = {1701.01875},
</span></span><span style="display:flex;"><span>  timestamp = {Wed, 07 Jun 2017 14:41:28 +0200},
</span></span><span style="display:flex;"><span>  biburl    = {https://dblp.org/rec/bib/journals/corr/CateDH17},
</span></span><span style="display:flex;"><span>  bibsource = {dblp computer science bibliography, https://dblp.org}
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
			</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2015-stanford-231a/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Violet: Optimal Image Selection with Machine Learning</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Kai-Yuan Neo </i> </div>
		
		<div class="research-venue"><a href='http://cs231a.stanford.edu'>CS231a: Computer Vision, Stanford, 2015</a></div>
		<div class="research-abstract">	
			People often capture several photos of the same scene to produce the best image. Manually choosing the best image out of the candidates is a time consuming process. We propose an algorithm to automatically detect the optimal image out of a set of candidate images. This eliminates the need for people to spend time evaluating the quality of their images, and allows them to focus on enjoying the memories they have experienced.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/unpublished-2015-stanford-231a/report.pdf">PDF</a> 
			
			
			<a class="research-action" href="/research/unpublished-2015-stanford-231a/slides.pdf">Slides</a> 
			
			
			
			
			
		</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2014-stanford-221/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>RTSift: Creating concise and meaningful review thread representations</b></div>
		<div class="research-authors"> <i> Kevin Chavez, <span class='highlight-author'>Fahim Dalvi</span> </i> </div>
		
		<div class="research-venue"><a href='http://cs221.stanford.edu'>CS221: Artificial Intelligence, Stanford, 2014</a></div>
		<div class="research-abstract">	
			We aim to produce a representation of review threads that is concise, interpretable, and preserves much of the meaning of the full text. Further, the representa- tion should be useful for various applications such as summarization, topic modeling, and star-rating pre- diction. This task can be modelled as a feature se- lection problem which consists of two stages: gen- erating feature proposals and filtering/ranking these features. The first stage automatically produces a large set of human-interpretable candidate features, while the second stage reduces that set to achieve a more concise representation.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/unpublished-2014-stanford-221/report.pdf">PDF</a> 
			<a class="research-action" href="/research/unpublished-2014-stanford-221/poster.pdf">Poster</a> 
			
			
			
			
			
			
			
		</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2013-met-qcri/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Multi-User Backend for Meeting Translation</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Francisco Guzman </i> </div>
		
		<div class="research-venue"><a href='http://qcri.org.qa'>Qatar Computing Research Institute, 2013</a></div>
		<div class="research-abstract">	
			The aim of the Meeting translation project is to provide a platform for multi-lingual meetings. In order for the system to work efficiently, a robust backend is required to augment the automatic recognition and translation services. The existing backend was a very simple proof-of-concept that supported a single user only. The goal of this project was to develop a backend that could support the realtime needs of this project. The backend was also required to support multiple users and meetings simultaneously. Another important aspect of the project was to test the robustness and efficiency of the system. Hence, a statistics collection system was also required that could give us enough information about the different processes in the pipeline to analyze and pin-point the deficiencies in the system. Upon completion, a fully working system was built that could support multiple users and meetings. The system integrated well with the translation and transcription services already available. The statistics collection system was also built and the results from the system were used to analyze the bottlenecks in the processing. The system was tested in both the English and the Arabic language.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/unpublished-2013-met-qcri/report.pdf">PDF</a> 
			<a class="research-action" href="/research/unpublished-2013-met-qcri/poster.pdf">Poster</a> 
			
			
			
			
			
			
			
		</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2013-i-want-my-mommy/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>I want my Mommy</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Syed Hashim Moosavi, Saquib Razak </i> </div>
		
		<div class="research-venue"><a href='https://issuu.com/carnegiemellonqatar/docs/mom_digest_2013_print/1'>Carnegie Mellon University Qatar, 2013</a></div>
		<div class="research-abstract">	
			“I want my Mommy” is a research project that aims to use wireless technologies such as Bluetooth and Wi-Fi to quickly locate people in a large crowd, subsequently reducing the number of lost people. In several crowded areas such as Makkah and Disneyland, people getting separated (specially children and elderly) from their families is a huge problem. This is currently handled manually by making announcements or giving people tags with information written on them. Unfortunately, these solutions do not work in highly crowded areas, both because of the number of people entering the location, and because of the size of these places. We plan to devise an algorithm using commonly existing wireless technologies to reduce the number of lost people by categorizing the crowd into groups without any barrier-to-entry.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			
			<a class="research-action" href="/research/unpublished-2013-i-want-my-mommy/poster.pdf">Poster</a> 
			
			
			
			
			
			
			
		</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2012-riss/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Airboats Data Visualizer</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Balajee Kannan, Paul Scerri </i> </div>
		
		<div class="research-venue"><a href='https://riss.ri.cmu.edu/research_showcase/2012-posters/'>Robotics Institute, Carnegie Mellon, 2012</a></div>
		<div class="research-abstract">	
			Small, autonomous watercraft are an ideal approach to a number of applications in flood mitigation and response, environmental sampling, and numerous other applications. Relative to other types of vehicles, watercraft are inexpensive, simple, robust and reliable.  The vision is to have large numbers of very inexpensive airboats provide situational awareness and deliver critical emergency supplies to victims, as well as low cost tools for environmental protection and monitoring. My role in this project was to create a visual interface to analyze and understand the data collected by the boats.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			
			<a class="research-action" href="/research/unpublished-2012-riss/poster.pdf">Poster</a> 
			
			
			
			
			
			
			
		</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2012-malware-inc/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Malware Inc - Web Browsers</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Baljit Singh, Thierry Sans </i> </div>
		
		<div class="research-venue"><a href='https://issuu.com/carnegiemellonqatar/docs/mom_digest_2012?backgroundColor=%2523222222'>Carnegie Mellon University Qatar, 2012</a></div>
		<div class="research-abstract">	
			Malware Inc. is a project that aims to study the development of Malware on various platforms, such as Web browsers, Social networks and Web engines. I chose Mozilla Firefox as my research platform, as it is the second most widely used web browser today. Having a wide audience adds to the importance of this research project, because the security of a higher number of people is at stake. My role in this research project was to study Firefox extensions, small pieces of code that help in enhancing the browsing experience. These extensions, although very helpful, have the potential to be used for malicious purposes.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			<a class="research-action" href="/research/unpublished-2012-malware-inc/report.pdf">PDF</a> 
			<a class="research-action" href="/research/unpublished-2012-malware-inc/poster.pdf">Poster</a> 
			
			<a class="research-action" href="/research/unpublished-2012-malware-inc/slides.pdf">Slides</a> 
			
			
			
			
			
		</div>
		
	</div>
</div>


	
	<div class="research-entry">
	<div class="research-thumb">
		<img src="/research/unpublished-2011-hala-interface/thumbnail.jpg">
	</div>
	<div class="research-details">
		<div class="research-title"><b>Integrating Natural Gestures in Touch Interfaces</b></div>
		<div class="research-authors"> <i> <span class='highlight-author'>Fahim Dalvi</span>, Ameer Abdulsalam, Majd Sakr </i> </div>
		
		<div class="research-venue"><a href='https://issuu.com/carnegiemellonqatar/docs/mom_digest_allpages?backgroundColor=%2523222222'>Carnegie Mellon University Qatar, 2011</a></div>
		<div class="research-abstract">	
			This work aims to explore the role of Natural Gestures in daily interaction with computer systems, in particular, their use in the navigation of touchscreen interfaces. Gestures provide a way for users to navigate an interface through intuitive on­screen touch motions and are leading to a shift from traditional point and click interactions to a more natural and physical way of interaction. Given the increased popularity of public touchscreen kiosks in various settings such as airports, hospitals and company lobbies, we designed and built a test­bed platform for exploring touchscreen interface design for users of mixed lingual and cultural backgrounds. Inspired by the increasing prominence of gestures in commercial touchscreen devices, our aim was to explore the effects of language and culture on gestures, including the impact of various aspects such as screen size on the usability and practicality of these gestures. We implemented a few of these gestures into our interface, such as natural scrolling, which enables the user to flick their fingers across the screen in order to browse through a list of items. As part of future work, we seek to implement additional gestures into the interface such as screen swiping and to deploy this system on a kiosk at Carnegie Mellon Qatar&#39;s campus for the purpose of collecting logs and running experiments. Through these experiments we seek to learn more about the interaction of users with the interface, their preferences and navigation performance, while considering the roles of language and culture in this region.
		</div>
		<div class="research-actions">
			<div class="research-action" onclick="this.parentNode.previousElementSibling.classList.toggle('expanded')">Abstract</div>
			
			<a class="research-action" href="/research/unpublished-2011-hala-interface/poster.pdf">Poster</a> 
			
			
			
			
			
			
			
		</div>
		
	</div>
</div>



</div>

  

</article>



      </main>

      <footer id='footer' class='footer'>
        <div class='container sep-before'><section class='widget widget-social_menu sep-after'><nav aria-label='Social Menu'>
    <ul><li>
        <a href='https://github.com/fdalvi' target='_blank' rel='noopener me'>
          <span class='screen-reader-text'>Open Github account in new tab</span><svg
  class="icon"
  xmlns="http://www.w3.org/2000/svg"
  viewbox="0 0 24 24"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  aria-hidden="true"
><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22" />
</svg>
</a>
      </li><li>
        <a href='mailto:dalvifahim&#43;website@gmail.com' target='_blank' rel='noopener me'>
          <span class='screen-reader-text'>Contact via Email</span><svg
  class="icon"
  xmlns="http://www.w3.org/2000/svg"
  viewbox="0 0 24 24"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  aria-hidden="true"
><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z" />
<polyline points="22,6 12,13 2,6" />
</svg>
</a>
      </li><li>
        <a href='https://linkedin.com/in/fdalvi' target='_blank' rel='noopener me'>
          <span class='screen-reader-text'>Open Linkedin account in new tab</span><svg
  class="icon"
  xmlns="http://www.w3.org/2000/svg"
  viewbox="0 0 24 24"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  aria-hidden="true"
><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z" />
<rect x="2" y="9" width="4" height="12" />
<circle cx="4" cy="4" r="2" />
</svg>
</a>
      </li><li>
        <a href='https://scholar.google.com/citations?user=uQGCv10AAAAJ' target='_blank' rel='noopener me'>
          <span class='screen-reader-text'>Open Google_scholar account in new tab</span><svg
  class="icon"
  xmlns="http://www.w3.org/2000/svg"
  viewbox="0 0 24 24"
  stroke-linecap="round"
  stroke-linejoin="round"
  stroke-width="2"
  aria-hidden="true"
><path d="M12 24a7 7 0 1 1 0-14 7 7 0 0 1 0 14zm0-24L0 9.5l4.838 3.94A8 8 0 0 1 12 9a8 8 0 0 1 7.162 4.44L24 9.5z" />
</svg>
</a>
      </li></ul>
  </nav>
</section><div class='copyright'>
  <p> &copy; 2017-2024 Fahim Dalvi. Powered by <a href="https://gohugo.io">Hugo</a> and <a href="https://github.com/MunifTanjim/minimo">Minimo</a></p>
</div>

        </div>
      </footer>

    </div>
  </div><script>window.__assets_js_src="/assets/js/"</script>

<script src='/assets/js/main.c3bcf2df.js'></script>

</body>

</html>

